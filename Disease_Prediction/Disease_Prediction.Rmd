---
title: "Disease Prediction using Machine Learning"
author: "Juilee"
date: "3/29/2020"
output: html_document
---
***Introduction:***
The objective of the assignment is to predict whether or not a patient has a certain unspecified disease.It is a binary classification problem.Multiple machine learning algorithms, including naive Bayes classifier, K Nearest Neighbor, Support Vector Machine (with both linear and non-linear kernel functions), Random Forest and Gradient Boosting Classifier to build a disease diagnosis model will be used.

Loading packages
```{r}
library(tidyverse)
library(ggplot2)
library(caret)
library(caretEnsemble)
library(rpart)
library(randomForest)
library(e1071)
library(klaR)
library(naivebayes)
library(doParallel)
library(Amelia)
library(pROC)
library(gridExtra)
library(grid)
library(resample)
library(ggpubr)
library(raster)
```

Loading training dataset.
```{r}
d_train<- read.csv("/Users/juilee81/Desktop/DA/DA_HW_03/Disease\ Prediction\ Training.csv",header=TRUE)
#View(d_train)
```

Checking for missing values column-wise and visualize the data:
```{r}
colSums(is.na(d_train))

#visualize the missing data
#missmap(d_train)
```

**Outlier detection**
```{r}
summary(d_train)
```
```{r fig.height=20, fig.width=10}
bxp_Age<- ggplot(d_train, aes(y=Age))+
              geom_boxplot(outlier.colour="red", 
              outlier.shape=16,
              outlier.size=2, notch=FALSE)

bxp_Height<- ggplot(d_train, aes(y=Height))+
              geom_boxplot(outlier.colour="red", 
              outlier.shape=16,
              outlier.size=2, notch=FALSE)

bxp_Weight<- ggplot(d_train, aes(y=Weight))+
              geom_boxplot(outlier.colour="red", 
              outlier.shape=16,
              outlier.size=2, notch=FALSE)

bxp_High.Blood.Pressure<- ggplot(d_train, aes(y=High.Blood.Pressure))+
              geom_boxplot(outlier.colour="red", 
              outlier.shape=16,
              outlier.size=2, notch=FALSE)

bxp_Low.Blood.Pressure<- ggplot(d_train, aes(y=Low.Blood.Pressure))+
              geom_boxplot(outlier.colour="red", 
              outlier.shape=16,
              outlier.size=2, notch=FALSE)



figure <- ggarrange(bxp_Age,bxp_Height,bxp_Weight,bxp_High.Blood.Pressure,bxp_Low.Blood.Pressure,
                    labels = c("A", "B"),
                    ncol = 2, nrow = 3)
figure
```
We will remove outliers from Height and Weight. Also will look to see 

```{r}
#install.packages("raster")
#Clamping using inter-quartile range
d_train$Height<-clamp(d_train$Height, lower=142.5, upper=186.5)
d_train$Weight<-clamp(d_train$Weight, lower=39.5, upper=107.5)
d_train$Low.Blood.Pressure<-clamp(d_train$Low.Blood.Pressure, lower=65, upper=90)
d_train$High.Blood.Pressure<-clamp(d_train$High.Blood.Pressure, lower=105, upper=170)

```
```{r fig.height=20, fig.width=10}
bxp_Age<- ggplot(d_train, aes(y=Age))+
              geom_boxplot(outlier.colour="red", 
              outlier.shape=16,
              outlier.size=2, notch=FALSE)

bxp_Height<- ggplot(d_train, aes(y=Height))+
              geom_boxplot(outlier.colour="red", 
              outlier.shape=16,
              outlier.size=2, notch=FALSE)

bxp_Weight<- ggplot(d_train, aes(y=Weight))+
              geom_boxplot(outlier.colour="red", 
              outlier.shape=16,
              outlier.size=2, notch=FALSE)

bxp_High.Blood.Pressure<- ggplot(d_train, aes(y=High.Blood.Pressure))+
              geom_boxplot(outlier.colour="red", 
              outlier.shape=16,
              outlier.size=2, notch=FALSE)

bxp_Low.Blood.Pressure<- ggplot(d_train, aes(y=Low.Blood.Pressure))+
              geom_boxplot(outlier.colour="red", 
              outlier.shape=16,
              outlier.size=2, notch=FALSE)



figure <- ggarrange(bxp_Age,bxp_Height,bxp_Weight,bxp_High.Blood.Pressure,bxp_Low.Blood.Pressure,
                    labels = c("A", "B"),
                    ncol = 2, nrow = 3)
figure
```
Removed outliers from four features.

Normalising the numeric data
```{r}
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x))) }
```
```{r}
d_train$Age <- normalize(d_train$Age)
d_train$Height <- normalize(d_train$Height)
d_train$Weight <- normalize(d_train$Weight)
d_train$Low.Blood.Pressure <- normalize(d_train$Low.Blood.Pressure)
d_train$High.Blood.Pressure  <- normalize(d_train$High.Blood.Pressure)


```


Exploratory Data Analysis:
```{r}
p1<-ggplot(d_train, aes(x=Cholesterol,  fill=Disease)) +
  geom_bar(stat="count",position="dodge")+theme_bw()
p2<-ggplot(d_train, aes(x=Glucose,  fill=Disease)) +
  geom_bar(stat="count",position="dodge")+theme_bw()
p3<-ggplot(d_train, aes(x=Smoke,  fill=Disease)) +
  geom_bar(stat="count",position="dodge")+theme_bw()
p4<-ggplot(d_train, aes(x=Exercise,  fill=Disease)) +
  geom_bar(stat="count",position="dodge")+theme_bw()
p5<-ggplot(d_train, aes(x=Gender,  fill=Disease)) +
  geom_bar(stat="count",position="dodge")+theme_bw()
grid.arrange(p1,p2,p3,p4,p5,ncol=3, nrow= 2)
```
```{r}
t1<-ggplot(d_train,aes(x=Age,fill=Cholesterol))+geom_density(col=NA,alpha=0.35)
t2<-ggplot(d_train,aes(x=Age,fill=Glucose))+geom_density(col=NA,alpha=0.35)

grid.arrange(t1,t2,ncol=2, nrow= 1)
```

Creating dummy variables of all the categorical variables
```{r}
library(fastDummies)
d_dummies <- fastDummies::dummy_cols(d_train,select_columns=c('Gender','Cholesterol','Glucose'))
d_dummies <- d_dummies[,c(-2,-7,-8)]
```

```{r}
d_dummies$Disease <-factor(d_dummies$Disease,labels = c("False", "True"))
```
This is necessary because our output will be in the form of 2 classes, True or False. Where true will denote that a patient has a disease and false denotes that a person is disease free.

```{r}

a<-ggplot(d_dummies, aes(d_train$Age, colour = Disease)) +
geom_freqpoly(binwidth = 1) + labs(title="Age Distribution by Disease")
a
```

```{r}
b<- d_dummies %>%
              ggplot(aes(x =Smoke, fill = Disease)) +  
              geom_density(alpha = 0.5) + 
              ggtitle("Smoke vs Disease") + 
              theme(plot.title = element_text(size =10),axis.text.x = element_text(size =7,angle = 45, hjust = 1),axis.title.x=element_blank())
b
```

```{r}
colnames(d_dummies)[14] <- "Cholesterol_too_high"
colnames(d_dummies)[17] <- "Glucose_too_high"
```
```{r}
c <- ggplot(d_dummies) + geom_histogram(aes(Cholesterol_too_high,fill=Disease), binwidth = 1, fill = "pink",col = "black")
c
```


```{r}
#Building a model
#split data into training and test data sets
indxTrain <- createDataPartition(y = d_dummies$Disease,p = 0.75,list = FALSE)
training <- d_dummies[indxTrain,]
testing <- d_dummies[-indxTrain,]
prop.table(table(d_dummies$Disease)) * 100
```
```{r}
testing$Disease <- factor(testing$Disease,labels = c("False", "True"))
```


---------------------------------------------------------------------------------------------------
**BUILDING MODELS**
**Logistic Regression**
```{r}
set.seed(123)
model_lr <- train(Disease ~ ., data = training, 
                   method = "glm", family = "binomial")

#print(model_lr)
```
```{r}
predict_lr <- predict(model_lr, newdata = testing)
confusionMatrix(predict_lr, testing$Disease)
```

*Regularization*
 **Elastic Net Regression**
Elastic net regression combines the properties of ridge and lasso regression. It works by penalizing the model using both the 1l2-norm1 and the 1l1-norm1. 
```{r}
set.seed(123)
# Set training control
train_cont <- trainControl(method = "repeatedcv",
                              number = 10,
                              repeats = 5,
                              search = "random",
                              verboseIter = TRUE)

# Train the model
elastic_reg <- train(Disease ~ .,
                           data = training,
                           method = "glmnet",
                           preProcess = c("center", "scale"),
                           tuneLength = 10,
                           trControl = train_cont)


# Best tuning parameter
elastic_reg$bestTune
```
The first line of code creates the training control object train_cont which specifies how the repeated cross validation will take place. The second line builds the elastic regression model in which a range of possible alpha and lambda values are tested and their optimum value is selected. The argument tuneLength specifies that 10 different combinations of values for alpha and lambda are to be tested.
```{r}
predict_lr <- predict(elastic_reg, newdata = testing)
confusionMatrix(predict_lr, testing$Disease)
```


**ANN0,1,2**
```{r}

```
```{r}
library(recipes)
rec_obj <- recipe(Disease ~ ., data = training) %>%
prep(data = training)
```
```{r}
set.seed(123)
library()
x_train_tbl <- bake(rec_obj, new_data = training)%>%dplyr::select(-Disease)
x_test_tbl <- bake(rec_obj, new_data = testing)%>% dplyr::select(-Disease)
y_train_vec <- ifelse(pull(training, Disease) == "True", 1, 0)
y_test_vec <-  ifelse(pull(testing, Disease) == "True", 1, 0)
str(x_train_tbl)
```

**0 hidden layer**
```{r}
library(keras)
set.seed(123)
model_keras0 <- keras_model_sequential()
model_keras0 %>%
layer_dense(units=1, kernel_initializer="uniform",activation = "sigmoid") %>%
compile(optimizer = "adam",loss = "binary_crossentropy",metrics = c("accuracy"))
model_keras0
model_keras0$layers

```
```{r}
set.seed(123)
library(forcats)
yhat_keras_class_vec <- predict_classes(object = model_keras, x = as.matrix(x_test_tbl)) %>%as.vector()
yhat_keras_prob_vec <- predict_proba(object = model_keras, x = as.matrix(x_test_tbl)) %>%as.vector()
estimates_keras_tbl <- tibble(truth = as.factor(y_test_vec)%>% fct_recode(true="1",false="0"),
                              estimate = as.factor(yhat_keras_class_vec)%>% fct_recode(true="1",false="0"),
                              class_prob = yhat_keras_prob_vec)

```
```{r}
library(yardstick)
options(yardstick.event_first = F)
estimates_keras_tbl %>% conf_mat(truth, estimate)
estimates_keras_tbl %>% metrics(truth, estimate)
```


**1 hidden layer**
```{r}
library(keras)
set.seed(123)
model_keras1 <- keras_model_sequential()
model_keras1 %>%
  layer_dense(units = 16,
              kernel_initializer = "uniform",
              activation = "relu",
              input_shape = ncol(x_train_tbl)) %>%
  layer_dense(units = 10,
              kernel_initializer = "uniform",
              activation = "relu") %>%
  layer_dense(units = 1, 
              kernel_initializer = "uniform",
              activation = "sigmoid") %>%
  compile(optimizer = "adam",
          loss = "binary_crossentropy",
          metrics = c("accuracy"))

model_keras1
model_keras1$layers
```
```{r}
set.seed(123)
library(forcats)
yhat_keras_class_vec <- predict_classes(object = model_keras1, x = as.matrix(x_test_tbl)) %>%as.vector()
yhat_keras_prob_vec <- predict_proba(object = model_keras1, x = as.matrix(x_test_tbl)) %>%as.vector()
estimates_keras_tbl <- tibble(
  truth = as.factor(y_test_vec)%>% fct_recode(true="1",false="0"),
  estimate = as.factor(yhat_keras_class_vec)%>% fct_recode(true="1",false="0"),
  class_prob = yhat_keras_prob_vec)
#estimates_keras_tbl
```
```{r}
library(yardstick)
options(yardstick.event_first = F)
estimates_keras_tbl %>% conf_mat(truth, estimate)
estimates_keras_tbl %>% metrics(truth, estimate)
```

**2 hidden layer**
```{r}
library(keras)
set.seed(123)
model_keras2 <- keras_model_sequential()
model_keras2 %>%
  layer_dense(units = 16,
              kernel_initializer = "uniform",
              activation = "relu",
              input_shape = ncol(x_train_tbl)) %>%
  layer_dense(units = 10,
              kernel_initializer = "uniform",
              activation = "relu") %>%
  layer_dense(units = 8,
              kernel_initializer = "uniform",
              activation = "relu") %>%
  layer_dense(units = 1, 
              kernel_initializer = "uniform",
              activation = "sigmoid") %>%
  compile(optimizer = "adam",
          loss = "binary_crossentropy",
          metrics = c("accuracy"))

model_keras2
model_keras2$layers

```
```{r}
set.seed(123)
library(forcats)

yhat_keras_class_vec <- predict_classes(object = model_keras2, x=as.matrix(x_test_tbl))%>%as.vector()
yhat_keras_prob_vec <- predict_proba(object = model_keras2, x = as.matrix(x_test_tbl)) %>%as.vector()
estimates_keras_tbl <- tibble(
  truth = as.factor(y_test_vec)%>% fct_recode(true="1",false="0"),
  estimate = as.factor(yhat_keras_class_vec)%>% fct_recode(true="1",false="0"),
  class_prob = yhat_keras_prob_vec)

```
```{r}
library(yardstick)
options(yardstick.event_first = F)
estimates_keras_tbl %>% conf_mat(truth, estimate)
estimates_keras_tbl %>% metrics(truth, estimate)
```

**Decision Tree**
Base model
```{r}
#install.packages("MLmetrics")
#library(MLmetrics)
dt_model = train(Disease ~ ., data = training, 
                  method="rpart")
dt_model
predict_dt <- predict(dt_model,newdata = testing)
confusionMatrix(predict_dt, testing$Disease)
```
Hyperparameter tuning Decision Tree
```{r}
library(caret)
library(rpart)
dt_model_tune <- train(Disease ~ ., data = training, 
                  metric = "Accuracy", 
                  method = "rpart",
                  tuneLength = 8,
                  trControl=trainControl("repeatedcv",number=10,repeats=10,classProbs=TRUE),
                  control =rpart.control(minsplit=50,minbucket=30,maxdepth=15,
                                         tuneGrid = expand.grid(cp = seq(0, 0.1, 0.02))))
```
Hyper-parameters:
minsplit: Set the minimum number of observations in the node before the algorithm perform a split
minbucket:  Set the minimum number of observations in the final note i.e. the leaf
maxdepth: Set the maximum depth of any node of the final tree. The root node is treated a depth 0
```{r}

predict_dt_tune <- predict(dt_model_tune,newdata = testing)
confusionMatrix(predict_dt_tune, testing$Disease)
#dt_model_postprune <- prune(dt_model_tune$finalModel)
#print(dt_model_postprune)
```
**Feature Importance**
```{r fig.height=5, fig.width=10}

grid.arrange(plot(varImp(elastic_reg),main="Logistic Regression"),plot(varImp(dt_model_tune),main="Decision Tree"),plot(varImp(model_rf_tune),main="Random Forest"),plot(varImp(model_gbm_tune),top=16 ,main="GBM"),nrow = 2, ncol = 2)
```
**Building a model : Naive Bayes Classifier**
```{r}
#newNBclassifier=naive_bayes(Disease ~.,usekernel=T,data=training)
#print(newNBclassifier)
set.seed(124) 
Naive_Bayes_Model <- naiveBayes(Disease ~.,data=training)

predict_nb <- predict(Naive_Bayes_Model, newdata = testing)
Distribution_of_diseased_vs_not_diseased<-table(predict_nb,testing$Disease)
confusionMatrix(predict_nb,testing$Disease)
```
The final output shows that the Naive Bayes classifier can predict whether a person has a disease or not, with an accuracy of approximately 68%.
```{r}
plot(Distribution_of_diseased_vs_not_diseased)
```
*Hyperparameter Tuning for naive bayes:*
```{r}
set.seed(124) 
Naive_Bayes_Model_tune <- naiveBayes(Disease ~.,data=training,laplace = 4,metric="Accuracy",type="prob")

predict_nb_tune <- predict(Naive_Bayes_Model_tune, newdata = testing)
confusionMatrix(predict_nb_tune,testing$Disease)
```
Laplace tuning: The goal is to increase the zero probability values to a small positive number.So that the posterior probabilities don't suddenly drop to zero.



**Building a model : KNN**
```{r}
set.seed(124) 
model_knn <- train(Disease ~ ., data = training, method = "knn")
```
```{r}
predict_knn <- predict(model_knn, newdata = testing)
confusionMatrix(predict_knn, testing$Disease)

```
*Hyperparameter Tuning for KNN:*
```{r}
pre_process <- preProcess(training, method = c("scale", "center"))
pre_process

```
```{r}
set.seed(124) 
model_knn_tune <- train(Disease ~ ., data =training, method = "knn",
                    tuneGrid = data.frame(k = seq(1, 25)),
                    trControl = trainControl(method = "repeatedcv",
                                             number = 3, repeats = 3))

model_knn_tune
```

1.**k:** The performance of KNNs is very sensitive to the choice of k.For high signal data with very few noisy (irrelevant) features, smaller values of k tend to work best. As more irrelevant features are involved, larger values of k are required to smooth out the noise.

```{r}
predict_knn_tune <- predict(model_knn_tune, newdata = testing)
confusionMatrix(predict_knn_tune,testing$Disease)
```
```{r}
plot(model_knn_tune)
```



**Building a model : SVM-Linear**
```{r SVM_L BASE}
set.seed(124) 
model_svm_linear <- train(Disease ~ ., data = training,method = "svmLinear")
model_svm_linear
```
```{r}
predict_svm_linear <- predict(model_svm_linear, newdata = testing)
confusionMatrix(predict_svm_linear, testing$Disease)

```

*Hyperparameter Tuning for SVM-Linear:*
```{r svm tune linear}

trctrl <- trainControl(method = "cv",number=3,repeats = 2)
set.seed(3233)
unwantedoutput1 <- capture.output(model_svm_linear_tune <- train(Disease ~ ., data =training,
                          method = "svmLinear",
                          metric="Accuracy",
                          trControl=trctrl,
                          tuneLength = 10,
                          tuneGrid = expand.grid(C = seq(0.5, 1, 2))))
print(model_svm_linear_tune)

```
```{r}
predict_svm_linear_tune <- predict(model_svm_linear_tune, newdata = testing)
confusionMatrix(predict_svm_linear_tune, testing$Disease)
```
1.For a linear kernel, the choice of C does not seem to affect performance very much.



**Building a model : Non linear SVM**
```{r svm non linear base}
set.seed(124) 
stratified_data <- training %>% sample_frac(0.5)
model_svm_rbf <- train(Disease ~ ., data = stratified_data,method="svmRadial")
model_svm_rbf
```
```{r}
predict_svm_rbf <- predict(model_svm_rbf, newdata = testing)
confusionMatrix(predict_svm_rbf, testing$Disease)

```
```{r}
plot(model_svm_rbf)
```
*Hyperparameter Tuning for SVM-Non-Linear:*
```{r}
set.seed(124) 
g <- expand.grid(sigma= seq(0.1,0.3,0.1), C= seq(0.5,1.5,0.5))
tr <- trainControl(method="cv",number = 3,repeats = 1)
```

```{r svm non linear tune}

model_svm_rbf_tune <- train(Disease ~ ., data = stratified_data,
                            metric='Accuracy',
                            tuneGrid = g ,
                            method ="svmRadial",
                            trControl=tr)
model_svm_rbf_tune
```

1. C: The C parameter controls how much you want to punish your model for each misclassified point for a given curve.Lower values of the C parameter allow the classifier to learn better under noisy data.
2. Sigma:Smaller sigma tends to be less bias and more variance while larger sigma tends to be less variance and more bias.Parameter defines how far the influence of a single training example reaches, with low values meaning ‘far’ and high values meaning ‘close’.

```{r}
predict_svm_rbf_tune <- predict(model_svm_rbf_tune, newdata = testing)
confusionMatrix(predict_svm_rbf_tune, testing$Disease)
```



**Building a model : Random forest**
```{r RF Base}
set.seed(124) 
model_rf <- train(Disease ~ ., data = training, method = "rf")
model_rf
```
```{r}
predict_rf <- predict(model_rf, newdata = testing)
confusionMatrix(predict_rf, testing$Disease)

```
```{r}
plot(model_rf)
varimp_rf <- varImp(model_rf)
varimp_rf
plot(varimp_rf, main = "Variable Importance with Random Forest")
```
*Hyperparameter Tuning for Random forest:*
```{r}
control <- trainControl(method="cv", number=3, repeats=1)
mtry <- c(1,2,5,10)
tunegrid <- expand.grid(.mtry=mtry)
```

Each axis of the grid is an algorithm parameter, and points in the grid are specific combinations of parameters. Because we are only tuning one parameter, the grid search is a linear search through a vector of candidate values.mtry parameter is available in caret for tuning.
1. **mtry:** Number of variables randomly sampled as candidates at each split.

```{r rf tune}
set.seed(124)
model_rf_tune <- train(Disease~., data=training, method="rf",metric='Accuracy',tuneGrid=tunegrid, trControl=control)
model_rf_tune
```
```{r}
predict_rf_tune <- predict(model_rf_tune, newdata = testing)
confusionMatrix(predict_rf_tune, testing$Disease)

```




**Building a model : Gradient Boosting**
```{r gbm base}
set.seed(124) 
unwantedoutput <- capture.output(model_gbm <- train(Disease ~ ., data = training, method = "gbm"))
```
```{r}
predict_gbm <- predict(model_gbm, newdata = testing)
confusionMatrix(predict_gbm, testing$Disease)

```
```{r}
plot(model_gbm)
```
*Hyperparameter Tuning for Gradient Boosting:*
```{r}
#install.packages("doParallel")
control <- trainControl(method="repeatedcv", number=3, repeats=3)
tgrid<- expand.grid(n.trees =c(1080:1100), 
                    interaction.depth=c(1:3), 
                    shrinkage=c(0.001,0.2,0.3), 
                    n.minobsinnode=15)
```

1.**n.trees:**
The total number of trees in the sequence or ensemble.Since they can easily overfit if there are many number of trees,we must find the optimal number of trees that minimize the loss function of interest with cross validation.
2.**shrinkage:**
Determines the contribution of each tree on the final outcome and controls how quickly the algorithm proceeds down the gradient descent.Generally, the smaller this value, the more accurate the model can be but also will require more trees in the sequence.
3.**interaction.depth:**
Controls the depth of the individual trees.Higher depth trees allow the algorithm to capture unique interactions but also increase the risk of over-fitting.
4.**n.minobsinnode:**
Controls the complexity of each tree.Higher values help prevent a model from learning relationships which might be highly specific to the particular sample selected for a tree (overfitting) but smaller values can help with imbalanced target classes in classification problems.

```{r gbm tune}
set.seed(124) #for reproducability
unwantedoutput <- capture.output(model_gbm_tune <- train(Disease ~ ., data = training, method = "gbm",metric="Accuracy",tuneGrid=tgrid,trControl=control))
print(model_gbm_tune)

```
```{r}
#install.packages("gbm")
library(gbm)

varImp(model_gbm_tune)
summary.gbm(model_gbm_tune)
predict_gbm_tune <- predict(model_gbm_tune, newdata = testing)
confusionMatrix(predict_gbm_tune, testing$Disease)
```


_____________________________________________________________________________________________________

***ROC AND AUC***
An ROC curve (receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification thresholds. 

#log reg
```{r}
log.probs <- predict(elastic_reg,testing,type="prob")
colnames(log.probs)[1]="False"
colnames(log.probs)[2]="True"
lr_roc_curve <- roc(testing$Disease,log.probs$True)
r4<-plot(lr_roc_curve)
#grid.arrange(r1,r2,r3,r4,nrow = 2, ncol = 2)
```
#Naive Bayes
```{r}

naive_bayes.probs <- predict(Naive_Bayes_Model_tune,testing,type="raw")
head(naive_bayes.probs)
colnames(naive_bayes.probs)[1]="False"
colnames(naive_bayes.probs)[2]="True"

naive_bayes_roc_curve <- roc(testing$Disease,naive_bayes.probs[, "True"])
r1<-plot(naive_bayes_roc_curve,col="blue")

```
#knn
```{r}

knn.probs <- predict(model_knn_tune,testing,type="prob")
head(knn.probs)
colnames(knn.probs)[1]="False"
colnames(knn.probs)[2]="True"
knn_roc_curve <- roc(testing$Disease,knn.probs$True)
r2<-plot(knn_roc_curve,col="green")

```

#Random forest
```{r}

rf.probs <- predict(model_rf_tune,testing,type="prob")
head(rf.probs)
colnames(rf.probs)[1]="False"
colnames(rf.probs)[2]="True"
rf_roc_curve <- roc(testing$Disease,rf.probs$True)
r3<-plot(rf_roc_curve)

```

#Gradient Boosting
```{r}
gbm.probs <- predict(model_gbm_tune,testing,type="prob")
head(gbm.probs)
colnames(gbm.probs)[1]="False"
colnames(gbm.probs)[2]="True"
gbm_roc_curve <- roc(testing$Disease,gbm.probs$True)
r4<-plot(gbm_roc_curve)
#grid.arrange(r1,r2,r3,r4,nrow = 2, ncol = 2)
grid.arrange(grobTree(plot(lr_roc_curve)),grobTree(plot(rf_roc_curve)),grobTree(plot(gbm_roc_curve)),nrow = 1,ncol = 3,top="Main Title")
```

***Area under the curve***
AUC provides an aggregate measure of performance across all possible classification thresholds.
A model whose predictions are 100% wrong has an AUC of 0.0; one whose predictions are 100% correct has an AUC of 1.0.
```{r}
auc_lr<-auc(testing$Disease, log.probs$True)
auc_naive_bayes<-auc(testing$Disease, naive_bayes.probs[, "True"])
auc_knn<-auc(testing$Disease, knn.probs$True)
auc_rf<-auc(testing$Disease, rf.probs$True)
auc_gbm<-auc(testing$Disease, gbm.probs$True)



```
```{r}
Model <- c("NB","KNN","Random_Forest","Gradient_boosting")
AUC <- c(auc_naive_bayes,auc_knn,auc_rf,auc_gbm,auc_lr)

auc<-data.frame(Model,AUC)
auc
```


________________________________________________________________________________________________
 
 SECTION 3: TESTING DATA



1. Prediction & Interpretation on Test Data
Reading the Testing data csv and storing it into a dataframe
```{r}
#Loading Weather Forecasting training dataset.

d_test <- read.csv("/Users/juilee81/Desktop/DA/DA_HW_03/Disease\ Prediction\ Testing.csv",header = TRUE)
#View(d_test)

```

Checking for missing values column-wise and visualize the data:
```{r}
colSums(is.na(d_test))
```

Normalising the numeric data
```{r}
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x))) }
```
```{r}
d_test$Age <- normalize(d_test$Age)
d_test$Height <- normalize(d_test$Height)
d_test$Weight <- normalize(d_test$Weight)
d_test$Low.Blood.Pressure <- normalize(d_test$Low.Blood.Pressure)
d_test$High.Blood.Pressure  <- normalize(d_test$High.Blood.Pressure)


```

Creating dummy variables of all the categorical variables
```{r}
library(fastDummies)
ID <- d_test$ID
d_dummies_test <- fastDummies::dummy_cols(d_test,select_columns=c('Gender','Cholesterol','Glucose'))
d_dummies_test <- d_dummies_test[,c(-1,-3,-8,-9)]
colnames(d_dummies_test)[13] <- "Cholesterol_too_high"
colnames(d_dummies_test)[16] <- "Glucose_too_high"

```
***Logistic model***
```{r}
#separating response and predictor variables

```

***Naive Bayes final model***
```{r}
predict_nb_final <- predict(Naive_Bayes_Model_tune, newdata = d_dummies_test)
```

***KNN final model***
```{r}
predict_knn_final <- predict(model_knn_tune, newdata = d_dummies_test)
```

***Random forest final model***
```{r}
predict_rf_final <- predict(model_rf_tune, newdata = d_dummies_test)
```

***Gradient Boosting Model***
```{r}
predict_gbm_final <- predict(model_gbm_tune, newdata = d_dummies_test)
```

***SVM-Linear final model***
```{r}
predict_svm_linear_final<-predict(model_svm_linear_tune, newdata = d_dummies_test)
```

***Non linear final model***
```{r}

predict_svm_non_linear_final <- predict(model_svm_rbf_tune, newdata = d_dummies_test)
```

***INTO CSV***
```{r csv}

Disease_predictions_csv <- data.frame(ID,predict_nb_final,predict_knn_final,predict_svm_linear_final,predict_svm_non_linear_final,predict_rf_final,predict_gbm_final) 
colnames(Disease_predictions_csv) <- c('ID','NB','KNN','SVM-Li','SVM-NLi','RF','GBM')
rownames(Disease_predictions_csv)<-NULL
```

*Writing to CSV*
```{r}

write.csv(Disease_predictions_csv,"HW_03_Juilee_Salunkhe_Predictions.csv")

```

***Conclusion***
In this study we have explored the data of unspecified disease dataset and gain insights about the key factors that decide the whether or not the person has the disease or not using multiple machine learning algorithms and data analysis.
