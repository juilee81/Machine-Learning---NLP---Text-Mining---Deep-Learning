{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Anger\n",
    "# 2 - Fear\n",
    "# 3 - Sad\n",
    "# 4 - Joy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as p\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import precision_score, classification_report, recall_score, confusion_matrix, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainanger=p.read_csv(\"EI-reg-En-anger-train.txt\", delimiter='\\t')\n",
    "trainfear=p.read_csv(\"EI-reg-En-fear-train.txt\", delimiter='\\t')\n",
    "trainsad=p.read_csv(\"EI-reg-En-sadness-train.txt\", delimiter='\\t')\n",
    "trainjoy=p.read_csv(\"EI-reg-En-joy-train.txt\", delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = p.concat([trainanger,trainfear,trainsad,trainjoy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Affect Dimension</th>\n",
       "      <th>Intensity Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2017-En-10264</td>\n",
       "      <td>@xandraaa5 @amayaallyn6 shut up hashtags are c...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2017-En-10072</td>\n",
       "      <td>it makes me so fucking irate jesus. nobody is ...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2017-En-11383</td>\n",
       "      <td>Lol Adam the Bull with his fake outrage...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2017-En-11102</td>\n",
       "      <td>@THATSSHAWTYLO passed away early this morning ...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2017-En-11506</td>\n",
       "      <td>@Kristiann1125 lol wow i was gonna say really?...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1611</td>\n",
       "      <td>2017-En-30316</td>\n",
       "      <td>Watch this amazing live.ly broadcast by @kana_...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1612</td>\n",
       "      <td>2017-En-31092</td>\n",
       "      <td>Watching @melissamccarthy in #Spy she's one of...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1613</td>\n",
       "      <td>2017-En-31037</td>\n",
       "      <td>Could not be happier!!</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1614</td>\n",
       "      <td>2017-En-31270</td>\n",
       "      <td>@strictlysimilak something about English spark...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1615</td>\n",
       "      <td>2017-En-31027</td>\n",
       "      <td>and I think some of our most spiritually weigh...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7102 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID                                              Tweet  \\\n",
       "0     2017-En-10264  @xandraaa5 @amayaallyn6 shut up hashtags are c...   \n",
       "1     2017-En-10072  it makes me so fucking irate jesus. nobody is ...   \n",
       "2     2017-En-11383         Lol Adam the Bull with his fake outrage...   \n",
       "3     2017-En-11102  @THATSSHAWTYLO passed away early this morning ...   \n",
       "4     2017-En-11506  @Kristiann1125 lol wow i was gonna say really?...   \n",
       "...             ...                                                ...   \n",
       "1611  2017-En-30316  Watch this amazing live.ly broadcast by @kana_...   \n",
       "1612  2017-En-31092  Watching @melissamccarthy in #Spy she's one of...   \n",
       "1613  2017-En-31037                            Could not be happier!!    \n",
       "1614  2017-En-31270  @strictlysimilak something about English spark...   \n",
       "1615  2017-En-31027  and I think some of our most spiritually weigh...   \n",
       "\n",
       "     Affect Dimension  Intensity Score  \n",
       "0               anger            0.562  \n",
       "1               anger            0.750  \n",
       "2               anger            0.417  \n",
       "3               anger            0.354  \n",
       "4               anger            0.438  \n",
       "...               ...              ...  \n",
       "1611              joy            0.558  \n",
       "1612              joy            0.780  \n",
       "1613              joy            0.885  \n",
       "1614              joy            0.360  \n",
       "1615              joy            0.440  \n",
       "\n",
       "[7102 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Affect Dimension'] = train['Affect Dimension'].map(dict(anger=1, fear=2, sadness=3, joy=4))\n",
    "y=train['Affect Dimension'].values\n",
    "X=train['Tweet'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4971,) (4971,) (2131,) (2131,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4]\n",
      "[502 685 469 475]\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(y_test, return_counts=True)\n",
    "print(unique)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MultiNomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[432  27  33  10]\n",
      " [ 30 604  39  12]\n",
      " [ 44  69 344  12]\n",
      " [ 22  26  14 413]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1      0.818     0.861     0.839       502\n",
      "           2      0.832     0.882     0.856       685\n",
      "           3      0.800     0.733     0.765       469\n",
      "           4      0.924     0.869     0.896       475\n",
      "\n",
      "    accuracy                          0.841      2131\n",
      "   macro avg      0.844     0.836     0.839      2131\n",
      "weighted avg      0.842     0.841     0.841      2131\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unigram_count_vectorizer = CountVectorizer(encoding='latin-1', binary=False, min_df=1,\n",
    "                                           stop_words='english', token_pattern='[a-z][a-z][a-z]+')\n",
    "X_train_vec = unigram_count_vectorizer.fit_transform(X_train)\n",
    "X_test_vec = unigram_count_vectorizer.transform(X_test)\n",
    "\n",
    "nb_clf= MultinomialNB()\n",
    "nb_clf.fit(X_train_vec,y_train)\n",
    "y_pred = nb_clf.predict(X_test_vec)\n",
    "cm=confusion_matrix(y_test, y_pred, labels=[1,2,3,4])\n",
    "print(cm)\n",
    "target_names = ['1','2','3','4']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names,digits = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[428  34  28  12]\n",
      " [ 24 619  35   7]\n",
      " [ 48  81 330  10]\n",
      " [ 21  30  15 409]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1      0.821     0.853     0.837       502\n",
      "           2      0.810     0.904     0.854       685\n",
      "           3      0.809     0.704     0.753       469\n",
      "           4      0.934     0.861     0.896       475\n",
      "\n",
      "    accuracy                          0.838      2131\n",
      "   macro avg      0.844     0.830     0.835      2131\n",
      "weighted avg      0.840     0.838     0.837      2131\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# bigram Count vectorizer, set minimum document frequency to 5\n",
    "bigram_count_vectorizer = CountVectorizer(encoding='latin-1', binary=False, min_df=1, ngram_range = (1,2),\n",
    "                                           stop_words='english', token_pattern='[a-z][a-z][a-z]+')\n",
    "X_train_vec = bigram_count_vectorizer.fit_transform(X_train)\n",
    "X_test_vec = bigram_count_vectorizer.transform(X_test)\n",
    "\n",
    "nb_clf= MultinomialNB()\n",
    "nb_clf.fit(X_train_vec,y_train)\n",
    "y_pred = nb_clf.predict(X_test_vec)\n",
    "cm=confusion_matrix(y_test, y_pred, labels=[1,2,3,4])\n",
    "print(cm)\n",
    "target_names = ['1','2','3','4']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names,digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[425  38  28  11]\n",
      " [ 23 620  35   7]\n",
      " [ 48  83 327  11]\n",
      " [ 22  30  15 408]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1      0.820     0.847     0.833       502\n",
      "           2      0.804     0.905     0.852       685\n",
      "           3      0.807     0.697     0.748       469\n",
      "           4      0.934     0.859     0.895       475\n",
      "\n",
      "    accuracy                          0.835      2131\n",
      "   macro avg      0.841     0.827     0.832      2131\n",
      "weighted avg      0.838     0.835     0.834      2131\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# trigram Count vectorizer, set minimum document frequency to 5\n",
    "trigram_count_vectorizer = CountVectorizer(encoding='latin-1', binary=False, min_df=1, ngram_range = (1,3),\n",
    "                                           stop_words='english', token_pattern='[a-z][a-z][a-z]+')\n",
    "X_train_vec = trigram_count_vectorizer.fit_transform(X_train)\n",
    "X_test_vec = trigram_count_vectorizer.transform(X_test)\n",
    "\n",
    "nb_clf= MultinomialNB()\n",
    "nb_clf.fit(X_train_vec,y_train)\n",
    "y_pred = nb_clf.predict(X_test_vec)\n",
    "cm=confusion_matrix(y_test, y_pred, labels=[1,2,3,4])\n",
    "print(cm)\n",
    "target_names = ['1','2','3','4']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names,digits = 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[443  30  25   4]\n",
      " [ 16 627  38   4]\n",
      " [ 34  46 380   9]\n",
      " [  6  27   8 434]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1      0.888     0.882     0.885       502\n",
      "           2      0.859     0.915     0.886       685\n",
      "           3      0.843     0.810     0.826       469\n",
      "           4      0.962     0.914     0.937       475\n",
      "\n",
      "    accuracy                          0.884      2131\n",
      "   macro avg      0.888     0.880     0.884      2131\n",
      "weighted avg      0.885     0.884     0.884      2131\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unigram Count vectorizer, set minimum document frequency to 5\n",
    "unigram_count_vectorizer = CountVectorizer(encoding='latin-1', binary=False, min_df=1, \n",
    "                                            stop_words='english',token_pattern='[a-z][a-z][a-z]+')\n",
    "X_train_vec = unigram_count_vectorizer.fit_transform(X_train)\n",
    "X_test_vec = unigram_count_vectorizer.transform(X_test)\n",
    "\n",
    "svm_clf = LinearSVC(C=0.1)\n",
    "svm_clf.fit(X_train_vec,y_train)\n",
    "y_pred = svm_clf.predict(X_test_vec)\n",
    "cm=confusion_matrix(y_test, y_pred, labels=[1,2,3,4])\n",
    "print(cm)\n",
    "target_names = ['1','2','3','4']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names,digits = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[451  20  26   5]\n",
      " [ 12 636  33   4]\n",
      " [ 35  46 380   8]\n",
      " [  6  19  10 440]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1      0.895     0.898     0.897       502\n",
      "           2      0.882     0.928     0.905       685\n",
      "           3      0.846     0.810     0.828       469\n",
      "           4      0.963     0.926     0.944       475\n",
      "\n",
      "    accuracy                          0.895      2131\n",
      "   macro avg      0.897     0.891     0.893      2131\n",
      "weighted avg      0.895     0.895     0.895      2131\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# bigram Count vectorizer, set minimum document frequency to 5\n",
    "bigram_count_vectorizer = CountVectorizer(encoding='latin-1', binary=False, min_df=1, ngram_range = (1,2),\n",
    "                                           stop_words='english', token_pattern='[a-z][a-z][a-z]+')\n",
    "X_train_vec = bigram_count_vectorizer.fit_transform(X_train)\n",
    "X_test_vec = bigram_count_vectorizer.transform(X_test)\n",
    "\n",
    "svm_clf = LinearSVC(C=0.1)\n",
    "svm_clf.fit(X_train_vec,y_train)\n",
    "y_pred = svm_clf.predict(X_test_vec)\n",
    "cm=confusion_matrix(y_test, y_pred, labels=[1,2,3,4])\n",
    "print(cm)\n",
    "target_names = ['1','2','3','4']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names,digits = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[451  21  25   5]\n",
      " [ 11 634  34   6]\n",
      " [ 34  50 376   9]\n",
      " [  6  19   9 441]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1      0.898     0.898     0.898       502\n",
      "           2      0.876     0.926     0.900       685\n",
      "           3      0.847     0.802     0.824       469\n",
      "           4      0.957     0.928     0.942       475\n",
      "\n",
      "    accuracy                          0.893      2131\n",
      "   macro avg      0.894     0.889     0.891      2131\n",
      "weighted avg      0.893     0.893     0.892      2131\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# trigram Count vectorizer, set minimum document frequency to 5\n",
    "trigram_count_vectorizer = CountVectorizer(encoding='latin-1', binary=False, min_df=1, ngram_range = (1,3),\n",
    "                                           stop_words='english', token_pattern='[a-z][a-z][a-z]+')\n",
    "X_train_vec = trigram_count_vectorizer.fit_transform(X_train)\n",
    "X_test_vec = trigram_count_vectorizer.transform(X_test)\n",
    "\n",
    "svm_clf = LinearSVC(C=0.2)\n",
    "svm_clf.fit(X_train_vec,y_train)\n",
    "y_pred = svm_clf.predict(X_test_vec)\n",
    "cm=confusion_matrix(y_test, y_pred, labels=[1,2,3,4])\n",
    "print(cm)\n",
    "target_names = ['1','2','3','4']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names,digits = 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The most hated and joyous words for MNB*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 anger words:\n",
      " [(-4.121765710437097, 'anger'), (-4.039073994591983, 'rage'), (-3.91045661676989, 'bitter'), (-3.8498319949534547, 'fuming'), (-3.8498319949534547, 'revenge'), (-3.7628206179638255, 'offended'), (-3.6162171437719497, 'outrage'), (-3.4126181885307103, 'madden'), (-3.4126181885307103, 'offend'), (-3.4126181885307103, 'sting')]\n",
      "\n",
      "Top 10 joyous words:\n",
      " [(3.35357352612964, 'breezy'), (3.3886648459409106, 'joyful'), (3.3886648459409106, 'rejoice'), (3.422566397616592, 'pleasing'), (3.4553562204395822, 'cheer'), (3.487104918754163, 'smiling'), (3.734941082658744, 'musically'), (3.759038634237804, 'glee'), (3.759038634237804, 'lively'), (4.0103530625187105, 'optimism')]\n"
     ]
    }
   ],
   "source": [
    "log_ratios = []\n",
    "features = unigram_count_vectorizer.get_feature_names()\n",
    "vneg_cond_prob = nb_clf.feature_log_prob_[0]\n",
    "vpos_cond_prob = nb_clf.feature_log_prob_[3]\n",
    "\n",
    "for i in range(0, len(features)):\n",
    "  log_ratio = vpos_cond_prob[i] - vneg_cond_prob[i]\n",
    "  log_ratios.append(log_ratio)\n",
    "\n",
    "exercise_C_ranks = sorted(zip(log_ratios, features))\n",
    "print('Top 10 anger words:\\n',exercise_C_ranks[:10])\n",
    "print('\\nTop 10 joyous words:\\n',exercise_C_ranks[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 anger words\n",
      "[(0.8968690446349229, 'angry'), (0.9062321576616679, 'offend'), (0.9243026107808726, 'offended'), (0.9268462738973147, 'furious'), (0.9601926917265275, 'snap'), (0.9702555501086053, 'madden'), (0.9715170605001081, 'rage'), (1.0472395348944064, 'bitter'), (1.0545588952246616, 'revenge'), (1.065822254594867, 'fuming')]\n",
      "\n",
      "Top 10 joyous words\n",
      "[(0.8695607449629719, 'smiling'), (0.8814337434027376, 'breezy'), (0.88379940342064, 'elated'), (0.8885026481329541, 'lively'), (0.9019140733061577, 'rejoicing'), (0.9181140131095553, 'cheer'), (0.9207691010252558, 'cheery'), (0.9661417167794855, 'glee'), (1.0251091529761611, 'optimism'), (1.0301524191390283, 'hilarious')]\n"
     ]
    }
   ],
   "source": [
    "feature_ranks = sorted(zip(svm_clf.coef_[0], bigram_count_vectorizer.get_feature_names()))\n",
    "\n",
    "very_negative_10 = feature_ranks[-10:]\n",
    "print(\"Top 10 anger words\")\n",
    "print([very_negative_10[i] for i in range(0, len(very_negative_10))])\n",
    "print()\n",
    "\n",
    "feature_ranks = sorted(zip(svm_clf.coef_[3], bigram_count_vectorizer.get_feature_names()))\n",
    "\n",
    "very_positive_10 = feature_ranks[-10:]\n",
    "print(\"Top 10 joyous words\")\n",
    "print([very_positive_10[i] for i in range(0, len(very_positive_10))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Error Analysis*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#ukedchat A4 Just go outside (or to the gym hall) and play! \\n #education  #learning\n",
      "@walterdonovanSS @mrb_rides_again evil, rich white men and their fucking cronies in intelligence/gov/academia using blithe blacks as fodder\n",
      "Just watched Django Unchained, Other people may frown, but I titter in delight! 2/5\n",
      "second day on the job and i already got a 45 dollar tip from a dude whose was constantly twitching his eye LOLOLOL \n",
      "@Casper10666 I assure you there is no laughter, but increasing anger at the costs, and arrogance of Westminster.\n",
      "Never make a #decision when you're #angry and never make a #promise when you're #happy. #wisewords\n",
      "\n",
      "errors: 6\n"
     ]
    }
   ],
   "source": [
    "err_cnt = 0\n",
    "for i in range(0, len(y_test)):\n",
    "    if(y_test[i]==4 and y_pred[i]==1):\n",
    "        print(X_test[i])\n",
    "        err_cnt = err_cnt+1\n",
    "print()\n",
    "print(\"errors:\", err_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB with TF vectors Score: 0.8514472926457929\n"
     ]
    }
   ],
   "source": [
    "nbTF_clf_pipe = Pipeline([('vect', CountVectorizer(encoding='latin-1', binary=False, min_df=1, stop_words='english', \n",
    "                                        token_pattern='[a-z][a-z][a-z]+')),('nbTF', MultinomialNB())])\n",
    "scores = cross_val_score(nbTF_clf_pipe, X, y, cv=6)\n",
    "avg=sum(scores)/len(scores)\n",
    "print('MultinomialNB with TF vectors Score:',avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM with TF vectors Score: 0.8997421948039239\n"
     ]
    }
   ],
   "source": [
    "svcTF_clf_pipe = Pipeline([('vect', CountVectorizer(encoding='latin-1', binary=False, min_df=1, stop_words='english', \n",
    "                                ngram_range = (1,2), token_pattern='[a-z][a-z][a-z]+')),('svcTF', LinearSVC(C=0.2))])\n",
    "scores = cross_val_score(svcTF_clf_pipe, X, y, cv=6)\n",
    "avg=sum(scores)/len(scores)\n",
    "print('SVM with TF vectors Score:',avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.05: 0.8897231346785547,\n",
       " 0.1: 0.8948850305021117,\n",
       " 0.15: 0.8948850305021117,\n",
       " 0.2: 0.8939465039887377,\n",
       " 0.25: 0.8939465039887377,\n",
       " 0.3: 0.8934772407320507,\n",
       " 0.35: 0.8934772407320507,\n",
       " 0.4: 0.8944157672454247,\n",
       " 0.45: 0.8939465039887377}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_count_vectorizer = CountVectorizer(encoding='latin-1', binary=False, min_df=1, ngram_range = (1,2),\n",
    "                                           stop_words='english', token_pattern='[a-z][a-z][a-z]+')\n",
    "X_train_vec = bigram_count_vectorizer.fit_transform(X_train)\n",
    "X_test_vec = bigram_count_vectorizer.transform(X_test)\n",
    "\n",
    "dic = {}\n",
    "\n",
    "for i in [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45]:\n",
    "    svm_clf = LinearSVC(C= i)\n",
    "    svm_clf.fit(X_train_vec,y_train)\n",
    "    y_pred = svm_clf.predict(X_test_vec)\n",
    "    dic[i] = accuracy_score(y_test,y_pred)\n",
    "\n",
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Likelihood:  -224792.77766766364\n",
      "Perplexity:  993.465857054348\n",
      "\n",
      "Topic 0:\n",
      "depression) (good) (anxiety) (life) (really) (sober) (having) (nightmare) (tonight) (morning) (feeling) (funny) (going) (panic) (pleasing) (years) (animated) (guys) (nervous) (future\n",
      "Topic 1:\n",
      "smile) (optimism) (start) (feel) (way) (like) (horror) (hilarious) (good) (face) (rejoice) (wanna) (delight) (heart) (hilarity) (little) (bright) (frown) (fuck) (beautiful\n",
      "Topic 2:\n",
      "want) (lost) (just) (sadness) (week) (awful) (afraid) (thing) (dreadful) (away) (pout) (gbbo) (dark) (terrorism) (don) (pakistan) (time) (hope) (friend) (shake\n",
      "Topic 3:\n",
      "amp) (know) (don) (bad) (man) (fucking) (old) (better) (like) (makes) (sparkling) (don know) (guy) (joy) (white) (place) (cause) (playful) (life) (took\n",
      "Topic 4:\n",
      "glee) (smiling) (hate) (cheer) (doesn) (just) (need) (unhappy) (bully) (news) (breezy) (stop) (exhilarating) (fuming) (hearty) (offended) (mean) (talk) (nice) (shy\n",
      "Topic 5:\n",
      "people) (just) (love) (make) (think) (don) (sad) (god) (best) (things) (lol) (really) (sure) (depressing) (world) (say) (life) (good) (rage) (black\n",
      "Topic 6:\n",
      "time) (let) (fear) (got) (laughter) (right) (despair) (work) (terror) (joyful) (wait) (twitter) (yeah) (pretty) (anger) (actually) (girl) (does) (new) (long\n",
      "Topic 7:\n",
      "day) (today) (watch) (live) (amazing) (night) (lively) (broadcast) (watch amazing) (amazing live) (live broadcast) (musically) (come) (thanks) (thank) (blues) (use) (trying) (game) (getting\n",
      "Topic 8:\n",
      "like) (look) (great) (just) (looking) (team) (person) (cheering) (tomorrow) (wish) (fun) (looks) (elated) (sadly) (chirp) (right) (high) (making) (comes) (eat\n",
      "Topic 9:\n",
      "happy) (did) (new) (didn) (shocking) (terrible) (home) (days) (just) (horrible) (end) (mind) (love) (follow) (birthday) (cheery) (big) (joyous) (grim) (angry\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "count_vectorizer = CountVectorizer(binary=False, stop_words='english', min_df=2, \n",
    "                                   token_pattern='[a-z][a-z][a-z]+', max_features = 1000, max_df = 0.95, ngram_range = (1,2))\n",
    "vecs = count_vectorizer.fit_transform(X)\n",
    "tf_feature_names = count_vectorizer.get_feature_names()\n",
    "\n",
    "no_topics = 10\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=no_topics, max_iter=5, learning_method='online', learning_offset=50.,\n",
    "                                random_state=0)\n",
    "lda_z = lda.fit_transform(vecs)\n",
    "\n",
    "print(\"Log Likelihood: \", lda.score(vecs))\n",
    "print(\"Perplexity: \", lda.perplexity(vecs))\n",
    "print()\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\") (\".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "display_topics(lda, tf_feature_names, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7102, 1000)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top #1: \n",
      "Someone needs to tell Candice she'll be stuck with that shitty pout if the wind changes. #GBBO …\n",
      "\n",
      "\n",
      "top #2: \n",
      "Someone needs to tell Candice she'll be stuck with that shitty pout if the wind changes. #GBBO …\n",
      "\n",
      "\n",
      "top #3: \n",
      "Some moving clips on youtube tonight of the vigil held at Tulsa Metropolitan Baptist church for #TerenceCruther #justice  #sadness …\n",
      "\n",
      "\n",
      "top #4: \n",
      "Pakistan is the biggest victim of terrorism - Nawaz Sharif \\nReally? It should have been biggest creator of terrorism. #UNGA …\n",
      "\n",
      "\n",
      "top #5: \n",
      "@mandyjohnson I'll be honest.. I hope that annoying Southern bint with the 'look at me' pout goes out this week! Selasi #FTW …\n",
      "\n",
      "\n",
      "top #6: \n",
      "@mandyjohnson I'll be honest.. I hope that annoying Southern bint with the 'look at me' pout goes out this week! Selasi #FTW …\n",
      "\n",
      "\n",
      "top #7: \n",
      "penny dreadful just cleaved off a fraction of my heart …\n",
      "\n",
      "\n",
      "top #8: \n",
      "penny dreadful just cleaved off a fraction of my heart …\n",
      "\n",
      "\n",
      "top #9: \n",
      "Saga: When all of your devices and teles fail just in time for bake off #panic #gbbo …\n",
      "\n",
      "\n",
      "top #10: \n",
      "@MagicAndFangs 'Just by getting lost! I don't want to see you in my eyes!' Hungary huffed and crossed her arms, looking away angrily. …\n",
      "\n",
      "\n",
      "top #11: \n",
      "Val is the type of woman you would dread bringing a donation for the bake sale #GBBO …\n",
      "\n",
      "\n",
      "top #12: \n",
      "Soooo badly want to dye my hair dark but have never been dark before soooo torn 😭😭😭 …\n",
      "\n",
      "\n",
      "top #13: \n",
      "@eileen_davidson on RHOBH, you just do not want to assume an affair while you were married so you criticize @LisaVanderpump! #awful …\n",
      "\n",
      "\n",
      "top #14: \n",
      "@pbump @AndyRichter  My heart goes out to that woman for the indignity of what she is sitting through. …\n",
      "\n",
      "\n",
      "top #15: \n",
      "Saga: When all of your devices and teles fail just in time for bake off  #gbbo …\n",
      "\n",
      "\n",
      "top #16: \n",
      "@twojacksdetail @bluelivesmtr Very rare an officer just shoots without regard. They don't want that on their conscience. #incite #inflame …\n",
      "\n",
      "\n",
      "top #17: \n",
      "@twojacksdetail @bluelivesmtr Very rare an officer just shoots without regard. They don't want that on their conscience. #incite  …\n",
      "\n",
      "\n",
      "top #18: \n",
      "@spamvicious I've just found out it's Candice and not Candace. She can pout all she likes for me 😍 …\n",
      "\n",
      "\n",
      "top #19: \n",
      "@spamvicious I've just found out it's Candice and not Candace. She can pout all she likes for me 😍 …\n",
      "\n",
      "\n",
      "top #20: \n",
      "It's been a week of awful connectivity with @TMobile no service or only 4G is not what Im paying for. #unhappy #poorservice …\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top = lda_z[:, 2].argsort()[::-1]\n",
    "for iter_idx, aqe_idx in enumerate(top[:20]):\n",
    "    print('top #%d: ' % (iter_idx + 1))\n",
    "    print(X[aqe_idx][:300], '…')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
