{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Anger\n",
    "# 2 - Fear\n",
    "# 3 - Sad\n",
    "# 4 - Joy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as p\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import precision_score, classification_report, recall_score, confusion_matrix, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainanger=p.read_csv(\"/Users/juilee81/Desktop/EI-reg-En-train/EI-reg-En-anger-train.txt\", delimiter='\\t')\n",
    "trainfear=p.read_csv(\"/Users/juilee81/Desktop/EI-reg-En-train/EI-reg-En-fear-train.txt\", delimiter='\\t')\n",
    "trainsad=p.read_csv(\"/Users/juilee81/Desktop/EI-reg-En-train/EI-reg-En-sadness-train.txt\", delimiter='\\t')\n",
    "trainjoy=p.read_csv(\"/Users/juilee81/Desktop/EI-reg-En-train/EI-reg-En-joy-train.txt\", delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = p.concat([trainanger,trainfear,trainsad,trainjoy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Affect Dimension</th>\n",
       "      <th>Intensity Score</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>users</th>\n",
       "      <th>hashtag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2017-En-10264</td>\n",
       "      <td>@xandraaa5 @amayaallyn6 shut up hashtags are c...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.562</td>\n",
       "      <td>[offended]</td>\n",
       "      <td>[xandraaa5, amayaallyn6]</td>\n",
       "      <td>[offended]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2017-En-10072</td>\n",
       "      <td>it makes me so fucking irate jesus. nobody is ...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.750</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2017-En-11383</td>\n",
       "      <td>Lol Adam the Bull with his fake outrage...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.417</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2017-En-11102</td>\n",
       "      <td>@THATSSHAWTYLO passed away early this morning ...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.354</td>\n",
       "      <td>[]</td>\n",
       "      <td>[THATSSHAWTYLO]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2017-En-11506</td>\n",
       "      <td>@Kristiann1125 lol wow i was gonna say really?...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.438</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Kristiann1125]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1611</td>\n",
       "      <td>2017-En-30316</td>\n",
       "      <td>Watch this amazing live.ly broadcast by @kana_...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.558</td>\n",
       "      <td>[musically]</td>\n",
       "      <td>[kana_blender]</td>\n",
       "      <td>[musically]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1612</td>\n",
       "      <td>2017-En-31092</td>\n",
       "      <td>Watching @melissamccarthy in #Spy she's one of...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.780</td>\n",
       "      <td>[Spy, theboss, hilarious]</td>\n",
       "      <td>[melissamccarthy]</td>\n",
       "      <td>[Spy, theboss, hilarious]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1613</td>\n",
       "      <td>2017-En-31037</td>\n",
       "      <td>Could not be happier!!</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.885</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1614</td>\n",
       "      <td>2017-En-31270</td>\n",
       "      <td>@strictlysimilak something about English spark...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.360</td>\n",
       "      <td>[]</td>\n",
       "      <td>[strictlysimilak]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1615</td>\n",
       "      <td>2017-En-31027</td>\n",
       "      <td>and I think some of our most spiritually weigh...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.440</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7102 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID                                              Tweet  \\\n",
       "0     2017-En-10264  @xandraaa5 @amayaallyn6 shut up hashtags are c...   \n",
       "1     2017-En-10072  it makes me so fucking irate jesus. nobody is ...   \n",
       "2     2017-En-11383         Lol Adam the Bull with his fake outrage...   \n",
       "3     2017-En-11102  @THATSSHAWTYLO passed away early this morning ...   \n",
       "4     2017-En-11506  @Kristiann1125 lol wow i was gonna say really?...   \n",
       "...             ...                                                ...   \n",
       "1611  2017-En-30316  Watch this amazing live.ly broadcast by @kana_...   \n",
       "1612  2017-En-31092  Watching @melissamccarthy in #Spy she's one of...   \n",
       "1613  2017-En-31037                            Could not be happier!!    \n",
       "1614  2017-En-31270  @strictlysimilak something about English spark...   \n",
       "1615  2017-En-31027  and I think some of our most spiritually weigh...   \n",
       "\n",
       "     Affect Dimension  Intensity Score                   hashtags  \\\n",
       "0               anger            0.562                 [offended]   \n",
       "1               anger            0.750                         []   \n",
       "2               anger            0.417                         []   \n",
       "3               anger            0.354                         []   \n",
       "4               anger            0.438                         []   \n",
       "...               ...              ...                        ...   \n",
       "1611              joy            0.558                [musically]   \n",
       "1612              joy            0.780  [Spy, theboss, hilarious]   \n",
       "1613              joy            0.885                         []   \n",
       "1614              joy            0.360                         []   \n",
       "1615              joy            0.440                         []   \n",
       "\n",
       "                         users                    hashtag  \n",
       "0     [xandraaa5, amayaallyn6]                 [offended]  \n",
       "1                           []                         []  \n",
       "2                           []                         []  \n",
       "3              [THATSSHAWTYLO]                         []  \n",
       "4              [Kristiann1125]                         []  \n",
       "...                        ...                        ...  \n",
       "1611            [kana_blender]                [musically]  \n",
       "1612         [melissamccarthy]  [Spy, theboss, hilarious]  \n",
       "1613                        []                         []  \n",
       "1614         [strictlysimilak]                         []  \n",
       "1615                        []                         []  \n",
       "\n",
       "[7102 rows x 7 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['hashtags'] = train['Tweet'].apply(lambda x: re.findall(r\"#(\\w+)\", x))\n",
    "train['users'] = train['Tweet'].apply(lambda x: re.findall(r\"@(\\w+)\", x))\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Affect Dimension'] = train['Affect Dimension'].map(dict(anger=1, fear=2, sadness=3, joy=4))\n",
    "y=train['Affect Dimension'].values\n",
    "X=train['Tweet'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4971,) (4971,) (2131,) (2131,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4]\n",
      "[1199 1567 1064 1141]\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(unique)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MultiNomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[407  39  39  17]\n",
      " [ 37 572  52  24]\n",
      " [ 46  60 346  17]\n",
      " [ 20  24  29 402]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1      0.798     0.811     0.804       502\n",
      "           2      0.823     0.835     0.829       685\n",
      "           3      0.742     0.738     0.740       469\n",
      "           4      0.874     0.846     0.860       475\n",
      "\n",
      "    accuracy                          0.810      2131\n",
      "   macro avg      0.809     0.807     0.808      2131\n",
      "weighted avg      0.811     0.810     0.811      2131\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unigram_count_vectorizer = CountVectorizer(encoding='latin-1', binary=False, min_df=5,\n",
    "                                           stop_words='english', token_pattern='[a-z][a-z][a-z]+')\n",
    "X_train_vec = unigram_count_vectorizer.fit_transform(X_train)\n",
    "X_test_vec = unigram_count_vectorizer.transform(X_test)\n",
    "\n",
    "nb_clf= MultinomialNB()\n",
    "nb_clf.fit(X_train_vec,y_train)\n",
    "y_pred = nb_clf.predict(X_test_vec)\n",
    "cm=confusion_matrix(y_test, y_pred, labels=[1,2,3,4])\n",
    "print(cm)\n",
    "target_names = ['1','2','3','4']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names,digits = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[408  40  38  16]\n",
      " [ 36 574  51  24]\n",
      " [ 46  62 344  17]\n",
      " [ 24  25  32 394]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.79      0.81      0.80       502\n",
      "           2       0.82      0.84      0.83       685\n",
      "           3       0.74      0.73      0.74       469\n",
      "           4       0.87      0.83      0.85       475\n",
      "\n",
      "    accuracy                           0.81      2131\n",
      "   macro avg       0.81      0.80      0.80      2131\n",
      "weighted avg       0.81      0.81      0.81      2131\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# bigram Count vectorizer, set minimum document frequency to 5\n",
    "bigram_count_vectorizer = CountVectorizer(encoding='latin-1', binary=False, min_df=5, ngram_range = (1,2),\n",
    "                                           stop_words='english', token_pattern='[a-z][a-z][a-z]+')\n",
    "X_train_vec = bigram_count_vectorizer.fit_transform(X_train)\n",
    "X_test_vec = bigram_count_vectorizer.transform(X_test)\n",
    "\n",
    "nb_clf= MultinomialNB()\n",
    "nb_clf.fit(X_train_vec,y_train)\n",
    "y_pred = nb_clf.predict(X_test_vec)\n",
    "cm=confusion_matrix(y_test, y_pred, labels=[1,2,3,4])\n",
    "print(cm)\n",
    "target_names = ['1','2','3','4']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[412  41  38  11]\n",
      " [ 36 576  51  22]\n",
      " [ 47  62 347  13]\n",
      " [ 27  30  35 383]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.79      0.82      0.80       502\n",
      "           2       0.81      0.84      0.83       685\n",
      "           3       0.74      0.74      0.74       469\n",
      "           4       0.89      0.81      0.85       475\n",
      "\n",
      "    accuracy                           0.81      2131\n",
      "   macro avg       0.81      0.80      0.80      2131\n",
      "weighted avg       0.81      0.81      0.81      2131\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# trigram Count vectorizer, set minimum document frequency to 5\n",
    "trigram_count_vectorizer = CountVectorizer(encoding='latin-1', binary=False, min_df=5, ngram_range = (1,3),\n",
    "                                           stop_words='english', token_pattern='[a-z][a-z][a-z]+')\n",
    "X_train_vec = trigram_count_vectorizer.fit_transform(X_train)\n",
    "X_test_vec = trigram_count_vectorizer.transform(X_test)\n",
    "\n",
    "nb_clf= MultinomialNB()\n",
    "nb_clf.fit(X_train_vec,y_train)\n",
    "y_pred = nb_clf.predict(X_test_vec)\n",
    "cm=confusion_matrix(y_test, y_pred, labels=[1,2,3,4])\n",
    "print(cm)\n",
    "target_names = ['1','2','3','4']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[410  53  25  14]\n",
      " [ 32 596  41  16]\n",
      " [ 40  58 361  10]\n",
      " [  9  35  17 414]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1      0.835     0.817     0.826       502\n",
      "           2      0.803     0.870     0.835       685\n",
      "           3      0.813     0.770     0.791       469\n",
      "           4      0.912     0.872     0.891       475\n",
      "\n",
      "    accuracy                          0.836      2131\n",
      "   macro avg      0.841     0.832     0.836      2131\n",
      "weighted avg      0.837     0.836     0.836      2131\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unigram Count vectorizer, set minimum document frequency to 5\n",
    "unigram_count_vectorizer = CountVectorizer(encoding='latin-1', binary=False, min_df=5, \n",
    "                                           stop_words='english', token_pattern='[a-z][a-z][a-z]+')\n",
    "X_train_vec = unigram_count_vectorizer.fit_transform(X_train)\n",
    "X_test_vec = unigram_count_vectorizer.transform(X_test)\n",
    "\n",
    "svm_clf = LinearSVC(C=0.2)\n",
    "svm_clf.fit(X_train_vec,y_train)\n",
    "y_pred = svm_clf.predict(X_test_vec)\n",
    "cm=confusion_matrix(y_test, y_pred, labels=[1,2,3,4])\n",
    "print(cm)\n",
    "target_names = ['1','2','3','4']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names,digits = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[411  51  25  15]\n",
      " [ 34 594  42  15]\n",
      " [ 40  57 362  10]\n",
      " [  9  37  14 415]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1      0.832     0.819     0.825       502\n",
      "           2      0.804     0.867     0.834       685\n",
      "           3      0.817     0.772     0.794       469\n",
      "           4      0.912     0.874     0.892       475\n",
      "\n",
      "    accuracy                          0.836      2131\n",
      "   macro avg      0.841     0.833     0.836      2131\n",
      "weighted avg      0.838     0.836     0.836      2131\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# bigram Count vectorizer, set minimum document frequency to 5\n",
    "bigram_count_vectorizer = CountVectorizer(encoding='latin-1', binary=False, min_df=5, ngram_range = (1,2),\n",
    "                                           stop_words='english', token_pattern='[a-z][a-z][a-z]+')\n",
    "X_train_vec = bigram_count_vectorizer.fit_transform(X_train)\n",
    "X_test_vec = bigram_count_vectorizer.transform(X_test)\n",
    "\n",
    "svm_clf = LinearSVC(C=0.2)\n",
    "svm_clf.fit(X_train_vec,y_train)\n",
    "y_pred = svm_clf.predict(X_test_vec)\n",
    "cm=confusion_matrix(y_test, y_pred, labels=[1,2,3,4])\n",
    "print(cm)\n",
    "target_names = ['1','2','3','4']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names,digits = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
