{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading Feedback\n",
    "96%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IST 718: Big Data Analytics\n",
    "\n",
    "- Professor: Willard Williamson <wewillia@syr.edu>\n",
    "- Faculty Assistant: Yash Pasar <yspasar@syr.edu>\n",
    "## General instructions:\n",
    "\n",
    "- You are welcome to discuss the problems with your classmates but __you are not allowed to copy any part of your answers from your classmates.  Short code snippets are allowed from the internet.  Code from the class text books or class provided code can be copied in its entirety.__\n",
    "- There could be tests in some cells (i.e., `assert` and `np.testing.` statements). These tests (if present) are used to grade your answers. **However, the professor and FAs could use __additional__ test for your answer. Think about cases where your code should run even if it passess all the tests you see.**\n",
    "- Before submitting your work, remember to check for run time errors with the following procedure:\n",
    "`Kernel`$\\rightarrow$`Restart and Run All`.  All runtime errors will result in a minimum penalty of half off.\n",
    "- Data Bricks is the official class runtime environment so you should test your code on Data Bricks before submission.  If there is a runtime problem in the grading environment, we will try your code on Data Bricks before making a final grading decision.\n",
    "- All plots shall include a title, and axis labels.\n",
    "- Grading feedback cells are there for graders to provide feedback to students.  Don't change or remove grading feedback cells.\n",
    "- Don't add or remove files from your git repo.\n",
    "- Do not change file names in your repo.  This also means don't change the title of the ipython notebook.\n",
    "- You are free to add additional code cells around the cells marked `your code here`.\n",
    "- Students may use toPandas() to print the head of data frames.\n",
    "- __Only use spark, spark machine learning, spark data frames, RDD's, and map reduce to solve all problems unless instructed otherwise.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not delete or change this cell\n",
    "\n",
    "import os\n",
    "\n",
    "# Define a function to determine if we are running on data bricks\n",
    "# Return true if running in the data bricks environment, false otherwise\n",
    "def is_databricks():\n",
    "    # get the databricks runtime version\n",
    "    db_env = os.getenv(\"DATABRICKS_RUNTIME_VERSION\")\n",
    "    \n",
    "    # if running on data bricks\n",
    "    if db_env != None:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Define a function to read the data file.  The full path data file name is constructed\n",
    "# by checking runtime environment variables to determine if the runtime environment is \n",
    "# databricks, or a student's personal computer.  The full path file name is then\n",
    "# constructed based on the runtime env.\n",
    "# \n",
    "# Params\n",
    "#   data_file_name: The base name of the data file to load\n",
    "# \n",
    "# Returns the full path file name based on the runtime env\n",
    "#\n",
    "def get_training_filename(data_file_name):    \n",
    "    # if running on data bricks\n",
    "    if is_databricks():\n",
    "        # build the full path file name assuming data brick env\n",
    "        full_path_name = \"/FileStore/tables/%s\" % data_file_name\n",
    "    # else the data is assumed to be in the same dir as this notebook\n",
    "    else:\n",
    "        # Assume the student is running on their own computer and load the data\n",
    "        # file from the same dir as this notebook\n",
    "        full_path_name = data_file_name\n",
    "    \n",
    "    # return the full path file name to the caller\n",
    "    return full_path_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as fn\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "from pyspark.ml import feature\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml import regression\n",
    "from pyspark.ml import classification\n",
    "from math import exp,fabs\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "import re \n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import IntegerType,DoubleType\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "Using the get_training_filename function defined in the cell above, read the sms_spam.csv file into a spark dataframe named spam_df.  There should be no empty columns in spam_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- type: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n",
      "+----+--------------------+\n",
      "|type|                text|\n",
      "+----+--------------------+\n",
      "| ham|Go until jurong p...|\n",
      "| ham|Ok lar... Joking ...|\n",
      "|spam|Free entry in 2 a...|\n",
      "| ham|U dun say so earl...|\n",
      "| ham|Nah I don't think...|\n",
      "+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "No of rows and columns in spam_df:(5574,2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "spam_df = get_training_filename('sms_spam.csv')\n",
    "spam_df = spark.read.csv(spam_df, header=True, inferSchema=True, mode=\"DROPMALFORMED\")\n",
    "spam_df.printSchema()\n",
    "spam_df.show(5)\n",
    "\n",
    "print(f\"No of rows and columns in spam_df:({spam_df.count()},{len(spam_df.columns)})\")\n",
    "spam_df.toPandas().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "Starting with spam_df, create a new dataframe named spam_df1.  Rename the spam_df type column to be named spam.  In the spam column, replace the string `spam` the with the integer 1 and the string `ham` with the integer 0.  Print the head and shape of spam_df1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|spam|                text|\n",
      "+----+--------------------+\n",
      "| ham|Go until jurong p...|\n",
      "| ham|Ok lar... Joking ...|\n",
      "|spam|Free entry in 2 a...|\n",
      "| ham|U dun say so earl...|\n",
      "+----+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "spam_df1 = spam_df\n",
    "spam_df1 = spam_df1.withColumnRenamed(\"type\",\"spam\")\n",
    "spam_df1.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_df1 = spam_df1.withColumn('spam',regexp_replace('spam', 'ham', '0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- spam: integer (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spam</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   spam                                               text\n",
       "0     0  Go until jurong point, crazy.. Available only ...\n",
       "1     0                      Ok lar... Joking wif u oni...\n",
       "2     1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3     0  U dun say so early hor... U c already then say...\n",
       "4     0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_df1 = spam_df1.withColumn('spam', regexp_replace('spam', 'spam', '1'))\n",
    "\n",
    "spam_df1 = spam_df1.withColumn(\"spam\",col(\"spam\").cast(IntegerType()))\n",
    "spam_df1.printSchema()\n",
    "spam_df1.toPandas().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "Starting with spam_df1, create a new dataframe named spam_df2 with a new column named filtered_text by removing stop words from the text column in spam_df.  Print the head and shape of spam_df2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.sql.utils import *\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.sql.functions import udf, col, lower, regexp_replace\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "\n",
    "\n",
    "spam_df_stop=spam_df1\n",
    "def clean_text(c):\n",
    "  c = lower(c)\n",
    "  c = regexp_replace(c, \"^rt \", \"\")\n",
    "  c = regexp_replace(c, \"(https?\\://)\\S+\", \"\")\n",
    "  c = regexp_replace(c, \"[^a-zA-Z0-9\\\\s]\", \"\")\n",
    "  \n",
    "  return c\n",
    "\n",
    "clean_text_df = spam_df_stop.select(clean_text(col(\"text\")).alias(\"text\"))\n",
    "# Tokenize text\n",
    "tokenizer = Tokenizer(inputCol='text', outputCol='text1')\n",
    "# Remove stop words\n",
    "remover = StopWordsRemover(inputCol='text1', outputCol=('filtered_text'))\n",
    "spam_df2 = spam_df1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of rows and columns in spam_df2:(5574,3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spam</th>\n",
       "      <th>text</th>\n",
       "      <th>filtered_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>[go, jurong, point,, crazy.., available, bugis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>[ok, lar..., joking, wif, u, oni...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>[u, dun, say, early, hor..., u, c, already, sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>[nah, think, goes, usf,, lives, around, though]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   spam                                               text  \\\n",
       "0     0  Go until jurong point, crazy.. Available only ...   \n",
       "1     0                      Ok lar... Joking wif u oni...   \n",
       "2     1  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3     0  U dun say so early hor... U c already then say...   \n",
       "4     0  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                       filtered_text  \n",
       "0  [go, jurong, point,, crazy.., available, bugis...  \n",
       "1               [ok, lar..., joking, wif, u, oni...]  \n",
       "2  [free, entry, 2, wkly, comp, win, fa, cup, fin...  \n",
       "3  [u, dun, say, early, hor..., u, c, already, sa...  \n",
       "4    [nah, think, goes, usf,, lives, around, though]  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_remover = Pipeline(stages=[tokenizer, remover])\n",
    "pipeline_transformer = pipeline_remover.fit(spam_df2)\n",
    "spam_df2 = pipeline_transformer.transform(spam_df2)\n",
    "                           \n",
    "spam_df2 = spam_df2.drop(\"text1\")\n",
    "\n",
    "print(f\"No of rows and columns in spam_df2:({spam_df2.count()},{len(spam_df2.columns)})\")\n",
    "spam_df2.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading Feedback\n",
    "-3 You didn't add any words to the stop word remover."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "Create a new dataframe named spam_df3 starting with spam_df2.  Create a new column named tfidf by performing a term frequency / inverse document frequency transformation on the filtered_text column of spam_df2.<br>  \n",
    "\n",
    "- Print the head and shape of spam_df3.  \n",
    "- Print the top 10 most important words indicated by the TFIDF score.  \n",
    "- Print the 10 least important words as indicated by the TFIDF score.\n",
    "- Print the total number of columns in the TFIDF data in spam_df3\n",
    "- Print the number of rows in the TFIDF data in spam_df3\n",
    "- Based only on the number of rows and columns in the TFIDF data, do you expect the model to overfit.  Explain your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+--------------------+\n",
      "|spam|                text|       filtered_text|               tfidf|\n",
      "+----+--------------------+--------------------+--------------------+\n",
      "|   0|Go until jurong p...|[go, jurong, poin...|(13361,[7,10,32,6...|\n",
      "|   0|Ok lar... Joking ...|[ok, lar..., joki...|(13361,[0,23,293,...|\n",
      "|   1|Free entry in 2 a...|[free, entry, 2, ...|(13361,[2,13,19,2...|\n",
      "|   0|U dun say so earl...|[u, dun, say, ear...|(13361,[0,71,78,1...|\n",
      "|   0|Nah I don't think...|[nah, think, goes...|(13361,[36,131,30...|\n",
      "+----+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import IDF as MLIDF\n",
    "\n",
    "\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"filtered_text\", outputCol=\"tf\")\n",
    "#tf = hashingTF.transform(spam_df3)\n",
    "idf = MLIDF(inputCol=\"tf\", outputCol=\"tfidf\")\n",
    "\n",
    "spam_df3 = spam_df2\n",
    "\n",
    "p_idf = Pipeline(stages=[cv,idf])\n",
    "p_transformer = p_idf.fit(spam_df3)\n",
    "\n",
    "spam_df3 = p_transformer.transform(spam_df3)\n",
    "spam_df3 = spam_df3.drop(\"tf\")\n",
    "print(spam_df3.show(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+--------------------+\n",
      "|spam|                text|       filtered_text|               tfidf|\n",
      "+----+--------------------+--------------------+--------------------+\n",
      "|   0|Go until jurong p...|[go, jurong, poin...|(13361,[7,10,32,6...|\n",
      "|   0|Ok lar... Joking ...|[ok, lar..., joki...|(13361,[0,23,293,...|\n",
      "|   1|Free entry in 2 a...|[free, entry, 2, ...|(13361,[2,13,19,2...|\n",
      "|   0|U dun say so earl...|[u, dun, say, ear...|(13361,[0,71,78,1...|\n",
      "|   0|Nah I don't think...|[nah, think, goes...|(13361,[36,131,30...|\n",
      "+----+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "1. Shape of spam_df3:(5574,4)\n",
      "2. Number of rows in the TFIDF data of spam_df3:(5574)\n",
      "3. Number of columns in the TFIDF data of spam_df3:(13361)\n",
      "\n",
      "Most important words:\n",
      "\n",
      "1. Word:grl:, tfidf score:7.9329004157683745\n",
      "2. Word:bloo, tfidf score:7.9329004157683745\n",
      "3. Word:dsn't, tfidf score:7.9329004157683745\n",
      "4. Word:Ã©, tfidf score:7.9329004157683745\n",
      "5. Word:pack, tfidf score:7.9329004157683745\n",
      "6. Word:boy:, tfidf score:7.9329004157683745\n",
      "7. Word:politicians., tfidf score:7.9329004157683745\n",
      "8. Word:favorite, tfidf score:7.9329004157683745\n",
      "9. Word:convinced, tfidf score:7.9329004157683745\n",
      "10. Word:num., tfidf score:7.9329004157683745\n",
      "\n",
      "\n",
      "Least important words:\n",
      "\n",
      "1. Word:u, tfidf score:2.026177097115483\n",
      "2. Word:call, tfidf score:2.3683800084456808\n",
      "3. Word:2, tfidf score:2.7125445906900496\n",
      "4. Word:get, tfidf score:2.751116865476289\n",
      "5. Word:ur, tfidf score:2.9024624943759387\n",
      "6. Word:, tfidf score:3.057703092567223\n",
      "7. Word:go, tfidf score:3.120716060395957\n",
      "8. Word:4, tfidf score:3.1664620821841605\n",
      "9. Word:got, tfidf score:3.2099471941238993\n",
      "10. Word:like, tfidf score:3.232420049975958\n"
     ]
    }
   ],
   "source": [
    "spam_df3.show(5)\n",
    "spam_df3.toPandas().head(5)\n",
    "print(f\"1. Shape of spam_df3:({spam_df3.count()},{len(spam_df3.columns)})\")\n",
    "print(f\"2. Number of rows in the TFIDF data of spam_df3:({spam_df3.select('tfidf').count()})\")\n",
    "print(f\"3. Number of columns in the TFIDF data of spam_df3:({len(p_transformer.stages[0].vocabulary)})\\n\")\n",
    "\n",
    "\n",
    "\n",
    "voc = p_transformer.stages[0].vocabulary\n",
    "tfidf_val = p_transformer.stages[1].idf.tolist()\n",
    "imp_words = {}\n",
    "for i in range(len(voc)):\n",
    "    imp_words[voc[i]] = tfidf_val[i]\n",
    "tf1 = sorted(imp_words.items(), key=lambda x: x[1], reverse=True)[0:10]\n",
    "tf2 = sorted(imp_words.items(), key=lambda x: x[1], reverse=False)[0:10]\n",
    "\n",
    "print(\"Most important words:\\n\")\n",
    "for i in range(10):\n",
    "    print(f\"{i + 1}. Word:{tf1[i][0]}, tfidf score:{tf1[i][1]}\")\n",
    "    \n",
    "print(\"\\n\")\n",
    "print(\"Least important words:\\n\")\n",
    "for i in range(10):\n",
    "    print(f\"{i + 1}. Word:{tf2[i][0]}, tfidf score:{tf2[i][1]}\")\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your model overfit explanation here: Since the number of rows are 5574 and number of features are 13361, the model will overfit (since no. of documents < no. of words). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "Create a pipeline named pipe1 capable of predicting ham or spam using logistic regression using spam_df3 as input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spam</th>\n",
       "      <th>text</th>\n",
       "      <th>filtered_text</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>rawPrediction</th>\n",
       "      <th>probability</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>[go, jurong, point,, crazy.., available, bugis...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.12071606...</td>\n",
       "      <td>[7.875126494636135, -7.875126494636135]</td>\n",
       "      <td>[0.9996200635299503, 0.0003799364700497243]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>[ok, lar..., joking, wif, u, oni...]</td>\n",
       "      <td>(2.026177097115483, 0.0, 0.0, 0.0, 0.0, 0.0, 0...</td>\n",
       "      <td>[7.175770351054618, -7.175770351054618]</td>\n",
       "      <td>[0.9992356883666598, 0.000764311633340185]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
       "      <td>(0.0, 0.0, 2.7125445906900496, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-4.6389541835295285, 4.6389541835295285]</td>\n",
       "      <td>[0.009575231636092262, 0.9904247683639077]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>[u, dun, say, early, hor..., u, c, already, sa...</td>\n",
       "      <td>(4.052354194230966, 0.0, 0.0, 0.0, 0.0, 0.0, 0...</td>\n",
       "      <td>[7.226854850811412, -7.226854850811412]</td>\n",
       "      <td>[0.9992737246826263, 0.000726275317373701]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>[nah, think, goes, usf,, lives, around, though]</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[7.1977172807977015, -7.1977172807977015]</td>\n",
       "      <td>[0.9992522675214567, 0.0007477324785433997]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "      <td>[freemsg, hey, darling, 3, week's, word, back!...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-4.131735649071952, 4.131735649071952]</td>\n",
       "      <td>[0.015801299119157366, 0.9841987008808426]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>[even, brother, like, speak, me., treat, like,...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[7.109576726863699, -7.109576726863699]</td>\n",
       "      <td>[0.9991834265022089, 0.0008165734977910332]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "      <td>[per, request, 'melle, melle, (oru, minnaminun...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[7.54669966676475, -7.54669966676475]</td>\n",
       "      <td>[0.9994724291436684, 0.0005275708563316142]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "      <td>[winner!!, valued, network, customer, selected...</td>\n",
       "      <td>(0.0, 2.3683800084456808, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-4.737612881155191, 4.737612881155191]</td>\n",
       "      <td>[0.008683468178936235, 0.9913165318210637]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "      <td>[mobile, 11, months, more?, u, r, entitled, up...</td>\n",
       "      <td>(2.026177097115483, 2.3683800084456808, 0.0, 0...</td>\n",
       "      <td>[-4.19809502732168, 4.19809502732168]</td>\n",
       "      <td>[0.014801785663550377, 0.9851982143364495]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   spam                                               text  \\\n",
       "0     0  Go until jurong point, crazy.. Available only ...   \n",
       "1     0                      Ok lar... Joking wif u oni...   \n",
       "2     1  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3     0  U dun say so early hor... U c already then say...   \n",
       "4     0  Nah I don't think he goes to usf, he lives aro...   \n",
       "5     1  FreeMsg Hey there darling it's been 3 week's n...   \n",
       "6     0  Even my brother is not like to speak with me. ...   \n",
       "7     0  As per your request 'Melle Melle (Oru Minnamin...   \n",
       "8     1  WINNER!! As a valued network customer you have...   \n",
       "9     1  Had your mobile 11 months or more? U R entitle...   \n",
       "\n",
       "                                       filtered_text  \\\n",
       "0  [go, jurong, point,, crazy.., available, bugis...   \n",
       "1               [ok, lar..., joking, wif, u, oni...]   \n",
       "2  [free, entry, 2, wkly, comp, win, fa, cup, fin...   \n",
       "3  [u, dun, say, early, hor..., u, c, already, sa...   \n",
       "4    [nah, think, goes, usf,, lives, around, though]   \n",
       "5  [freemsg, hey, darling, 3, week's, word, back!...   \n",
       "6  [even, brother, like, speak, me., treat, like,...   \n",
       "7  [per, request, 'melle, melle, (oru, minnaminun...   \n",
       "8  [winner!!, valued, network, customer, selected...   \n",
       "9  [mobile, 11, months, more?, u, r, entitled, up...   \n",
       "\n",
       "                                               tfidf  \\\n",
       "0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.12071606...   \n",
       "1  (2.026177097115483, 0.0, 0.0, 0.0, 0.0, 0.0, 0...   \n",
       "2  (0.0, 0.0, 2.7125445906900496, 0.0, 0.0, 0.0, ...   \n",
       "3  (4.052354194230966, 0.0, 0.0, 0.0, 0.0, 0.0, 0...   \n",
       "4  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "5  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "6  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "7  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "8  (0.0, 2.3683800084456808, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "9  (2.026177097115483, 2.3683800084456808, 0.0, 0...   \n",
       "\n",
       "                               rawPrediction  \\\n",
       "0    [7.875126494636135, -7.875126494636135]   \n",
       "1    [7.175770351054618, -7.175770351054618]   \n",
       "2  [-4.6389541835295285, 4.6389541835295285]   \n",
       "3    [7.226854850811412, -7.226854850811412]   \n",
       "4  [7.1977172807977015, -7.1977172807977015]   \n",
       "5    [-4.131735649071952, 4.131735649071952]   \n",
       "6    [7.109576726863699, -7.109576726863699]   \n",
       "7      [7.54669966676475, -7.54669966676475]   \n",
       "8    [-4.737612881155191, 4.737612881155191]   \n",
       "9      [-4.19809502732168, 4.19809502732168]   \n",
       "\n",
       "                                   probability  prediction  \n",
       "0  [0.9996200635299503, 0.0003799364700497243]         0.0  \n",
       "1   [0.9992356883666598, 0.000764311633340185]         0.0  \n",
       "2   [0.009575231636092262, 0.9904247683639077]         1.0  \n",
       "3   [0.9992737246826263, 0.000726275317373701]         0.0  \n",
       "4  [0.9992522675214567, 0.0007477324785433997]         0.0  \n",
       "5   [0.015801299119157366, 0.9841987008808426]         1.0  \n",
       "6  [0.9991834265022089, 0.0008165734977910332]         0.0  \n",
       "7  [0.9994724291436684, 0.0005275708563316142]         0.0  \n",
       "8   [0.008683468178936235, 0.9913165318210637]         1.0  \n",
       "9   [0.014801785663550377, 0.9851982143364495]         1.0  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "from pyspark.ml import Pipeline \n",
    "from pyspark.ml.feature import * \n",
    "from pyspark.ml.classification import LogisticRegression \n",
    "\n",
    "\n",
    "logistic = LogisticRegression(labelCol=\"spam\",featuresCol =\"tfidf\",maxIter=10, regParam=0.01)\n",
    "pipe1 = Pipeline(stages=[logistic]) \n",
    "# Fit the pipeline \n",
    "model = pipe1.fit(spam_df3) \n",
    "prediction = model.transform(spam_df3)\n",
    "prediction.toPandas().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5\n",
    "Fit pipe1 using a [CrossValidator](https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.tuning.CrossValidator) object with the number of cross validation folds = 3.  Score the model using a [BinaryClassificationEvaluator](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html) using ROC AUC as the metric.  Name the cross validator object cv1 and the fitted cross validator object fitted_cv1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "from pyspark.ml.tuning import ParamGridBuilder,CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "\n",
    "grid = ParamGridBuilder().build()\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='spam', metricName='areaUnderROC')\n",
    "cv1 = CrossValidator(estimator=pipe1, estimatorParamMaps=grid,numFolds=3,evaluator=evaluator,seed=70)\n",
    "fitted_cv1 = cv1.fit(spam_df3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6\n",
    "Print the cross validation AUC score from fitted_cv1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC Curve: 1.0\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "roc = evaluator.evaluate(fitted_cv1.transform(spam_df3))\n",
    "print(f\"Area under ROC Curve: {roc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading Feedback\n",
    "-3 We asked for the cross validation AUC and you computed an AUC using training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7\n",
    "Create a ROC scatter plot from fitted_pipe1 TPR/FPR data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUoAAAFNCAYAAABmLCa9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deXxU9dX48c/JQhYSCEvYwhJ2AoEECVStP1EQoT4u1N26oMVarctTt9atT1ttq49aba12oT4Wbd3qhmi1WBS1tbIJ2VgTNkmgEJZAAtnn/P6YSQwhyUzCTO4s5/16zcu5d75z77khntzv/d77PaKqGGOMaVuU0wEYY0yws0RpjDFeWKI0xhgvLFEaY4wXliiNMcYLS5TGGOOFJUpjjPHCEqUJKBHZLiJVIlIpIv8RkYUiktSizaki8pGIVIjIIRF5R0TGt2jTQ0R+JSJferZV7Fnu28Z+RURuE5FCETkiIiUi8pqITAzk8ZrwZInSdIXzVDUJyAYmA/c2fiAipwAfAG8Dg4DhQB7wmYiM8LTpBnwITADmAD2AU4H9wLQ29vlr4L+B24DewBhgEfBfHQ1eRGI6+h0TZlTVXvYK2AvYDpzVbPlR4G/Nlv8J/LaV770PvOB5fz2wB0jycZ+jgQZgWjttPgaub7Z8LfCvZssK3AwUAduA3wOPt9jG28AdnveDgDeAMk/725z+2dvLfy87ozRdRkQGA98Aij3LibjPDF9rpflfgVme92cBf1fVSh93NRMoUdWVJxYxc4GvAeOBl4DLREQARKQXcDbwiohEAe/gPhNO8+z/+yIy+wT3b4KEJUrTFRaJSAWwE9gL/Nizvjfu38HdrXxnN9B4/bFPG23a0tH2bXlYVQ+oahXuM18F/p/ns4uBz1V1FzAVSFXVB1W1VlW3An8ELvdDDCYIWKI0XWGuqiYDZwDj+CoBHgRcwMBWvjMQ2Od5v7+NNm3paPu27Gx8o+7+9SvAFZ5V3wJe9LwfBgwSkfLGF3Af0N8PMZggYInSdBlV/QRYCDzuWT4CfA5c0krzS3EP4AAsBWaLSHcfd/UhMFhEctppcwRIbLY8oLWQWyy/DFwsIsNwd8nf8KzfCWxT1ZRmr2RVPcfHeE2Qs0RputqvgFkiku1ZvgeY57mVJ1lEeonIz4BTgJ962vwZdzJ6Q0TGiUiUiPQRkftE5LhkpKpFwG+Bl0XkDBHpJiLxInK5iNzjaZYLXCgiiSIyCpjvLXBVXYt7sOZZYImqlns+WgkcFpEfikiCiESLSKaITO3MD8gEH0uUpkupahnwAvAjz/K/gNnAhbivK+7AfQvRaZ6Eh6rW4B7Q2Qj8AziMOzn1BVa0savbgKeBZ4ByYAvwTdyDLgBPArW4R9Of56tutDcve2J5qdkxNQDn4b79aRvuSwbPAj193KYJcuK+9GKMMaYtdkZpjDFeWKI0xhgvLFEaY4wXliiNMcYLS5TGGONFyM2K0rdvX01PT3c6DGNMmPniiy/2qWpqa5+FXKJMT09n9erVTodhjAkzIrKjrc+s622MMV5YojTGGC8sURpjjBeWKI0xxgtLlMYY44UlSmOM8cISpTHGeGGJ0hhjvLBEaYwxXgQsUYrIcyKyV0QK2/hcROQpESkWkXwROSlQsRhjzIkI5COMC3FPxf9CG59/A3eh+tG4CzX9zvPfgFq0tpTHlmyitLyKaBEaVElJiEUEyo/W0bOV9weP1rXadlBKAmeOS2XZxjJ2lVcxKCWBu2ePBejwPnx531YcJ9rWyTg68nlai5+3v/YRqNhDrW0wxHgisTeKFuGKrw3hZ3Mn+i1vBLQUhIikA++qamYrn/0B+FhVX/YsbwLOUNV26zHn5ORoZ5/1XrS2lHvfLKCqrqFT3/dFbJSAQF2DldgwxklXnTy0Q8lSRL5Q1VYrdzp5jTKNZnWTgRLPuoB5bMmmgCZJgDqXWpI0povFRx9lRM9Nx6x7ecXONlp3nJOJUlpZ12qGEZEbRGS1iKwuKyvr9A53lVd1+rvGmOCUGFPJ3VMf4M6c/yExprJpfYMfe8tOJsoSYEiz5cHArtYaquoCVc1R1ZzU1Fani/PJoJSETn/XGBN8usce5gdT72dYj608W3A7R+uTmj6LltbOxTrHyUS5GLjGM/p9MnDI2/XJE3X37LEkxEYHchfERgmx0f77BzLGtC42qpYfTr2PtKQv+fWaB1i79+RjPr/ia0Pa+GbHBWzUW0ReBs4A+opICfBjIBZAVX8PvAecAxQDR4HrAhVL40h340hpfGyUX0btbNTbRr2dHiWO7FFv+GLvbP5zNJ2CfV8N2oTcqHcgdHTUu7WR7oTYaB6+cCJzJwd07MgYEwA1NaXU1JTQo4d/7yYM1lHvLtHaSHdVXQOPLdnUxjeMMcGqunoHa9eezrp1F+Ny1XTZfkOuZk5HtTXSbSPgxoSWqqqt5OaeSUPDYSZNWkJUVFyX7TvszyjbGum2EXBjQsfRo5tZu/Z0Ghoqycr6kB49pnXp/sM+UbY20p0QG9006GKMCX4lJb9CtZbs7I9JTu76aSHCvuvdOGDTOOrdODJtAznGBD9VRUQYNerXDBlyNwkJwx2JI+wTJbiTpSVGY0JLRcUaiou/z4QJr9OtWz/HkiRESKI0xoSWw4dXkp8/m+joHjQ0VAL9HI0n7K9RGmNCy6FDn5GXdxYxMb2ZPPlTEhJGOB2SJUpjTPA4dOjf5OXNplu3gWRnf0J8/DCnQwIsURpjgkhCwkh69z6b7OyPiY8f7HQ4TSxRGmMcd/jwKlyuOrp1609m5pvExQ10OqRjWKI0xjhq377FrF17Gjt2POh0KG2yRGmMcczeva+zbt1FJCVlM3jwHU6H0yZLlMYYR+zZ8xLr119OcvI0srL+QWxsL6dDapMlSmNMl6urO0hR0c307HkakyYtISamh9MhtSsibjhvPnGvPcJojPNiY3uRlfURiYljiY5OdDocr8I+UbacuLe0vIp73ywAsGRpTBcrLX0Gl6uWIUNuJzl5stPh+Czsu942ca8xwWHnzicpKrqF8vJPUHU5HU6HhH2itIl7jXHejh2PsGXLHaSmXsyECa8hElqpJ7Si7QSbuNcYZ23f/hDbtt1Lv37fIiPjZaKiYp0OqcPCPlHaxL3GOCs2tjcDBlxLRsYLREWF5rBIaEbdATZxrzFdT1WpqiomMXE0aWk3N03AG6rCPlGCTdxrTFdSdVFc/N/s3v0ncnJySUwcFdJJEiKg622M6TqqLjZvvpHS0qdJS7uJhISRTofkFxFxRmmMCTzVBjZunM+ePc8zdOj9DB/+UMifSTayRGmM8Yvdu59lz57nSU9/kPT0Hzkdjl9ZojTG+MWAAfPp1m0Affte4HQofmfXKI0xneZy1VBUdBs1NbuIiooJyyQJliiNMZ3U0FBNYeGFlJb+hoMHP3I6nICyrrcxpsMaGo5SWHgBBw9+yJgxCxgw4CqnQwooS5TGmA6pr6+ksPA8yss/Zdy4PzFgwDynQwo4S5TGmA5RraG+/hAZGX+hf/8rnA6nS1iiNMb4pL7+EFFR8cTG9uGkk1aG7HPbnWGDOcYYr+rq9pObO4MNG64BiKgkCRFyRmmlIIzpvNraveTlzeLo0U0MH/4zp8NxRNgnSisFYUzn1dTsJi/vLKqrtzFx4rv07n2W0yE5Iuy73lYKwpjOUVUKC79JdfUOJk16P2KTJETAGaWVgjCmc0SE0aN/g2otPXt+3elwHBX2Z5RWCsKYjqmq2kpJydMA9OgxNeKTJERAorRSEMb47ujRzaxdezrbt/+Y2tq9TocTNAKaKEVkjohsEpFiEbmnlc+HisgyEVkrIvkico6/Y5g7OY2HL5xIWkoCAqSlJPDwhRNtIMeYFo4cWU9u7nRUa8nOXka3bv2cDiloiKoGZsMi0cBmYBZQAqwCrlDV9c3aLADWqurvRGQ88J6qpre33ZycHF29enVAYjYmUlVW5pOXdxYi0WRlfUj37uOdDqnLicgXqprT2meBPKOcBhSr6lZVrQVeAVrOwaRAD8/7nsCuAMZjjGlDZWUuUVFxZGd/EpFJ0ptAjnqnATubLZcAX2vR5ifAByJyK9AdaPX+AxG5AbgBYOjQoX4P1JhI1dBwlOjoRAYMuIbU1IuIju7udEhBKZBnlK0Vy2jZz78CWKiqg4FzgD+LyHExqeoCVc1R1ZzU1NQAhGpM5Dl06DOWLx9OefknAJYk2xHIRFkCDGm2PJjju9bzgb8CqOrnQDzQN4AxGWOAgwc/Ji9vNjExKSQkjHI6nKAXyES5ChgtIsNFpBtwObC4RZsvgZkAIpKBO1GWBTAmYyLegQNLKSg4h/j4YWRnf0JcnN0B4k3AEqWq1gO3AEuADcBfVXWdiDwoIud7mt0JfEdE8oCXgWs1UMPwxhgqK/MpKDiXhIRRZGcvIy5ugNMhhYSA3R4UKHZ7kDGdp+pix46fk5b2PWJj+zgdTlBx6vYgY0yQ2LdvMdXVOxCJIj39R5YkO8gSpTFhbs+elygs/Cbbtj3gdCghyxKlMWFs9+6FbNhwFSkppzN69O+cDidkWaI0Jkzt2rWATZuuo1evs5g48W/ExCQ5HVLIskRpTBhyuerYtWsBvXufQ2bmYqKjE50OKaSF/cS9YDVzTGRRbSAqKpasrH8QHZ1IVFSc0yGFvLA/o2ysmVNaXoXyVc2cRWtLnQ7NGL/bseNhCgrOxeWqITa2lyVJPwn7RGk1c0wkUFW2b/8p27bdR0xMHyDa63eM78K+6201c0y4U1W2bbufL798mAEDrmXs2GdxTwdr/CXszyitZo4Jdzt2PMiXXz7MwIHfZezY/7MkGQBhnyitZo4Jd336XMDQofcyZszvaGWWQuMHYd/1bhzdtlFvE05UXezf/y59+55PcnI2ycnZTocU1sI+UYI7WVpiNOFCtYFNm67nP/9ZSFbWR/TqdabTIYW9iEiUxoQLl6uejRvnsXfvS6Sn/5SUlDOcDikiWKI0JkS4XHVs2HAlZWWvMXz4wwwbdlwFaBMgliiNCRGHDv2LsrI3GDnyCYYMud3pcCKKJUpjgpyqIiL06nUmU6euo3v3cU6HFHHsXgJjglhDw1EKCs5l//73ACxJOsQSpTFBqr6+kvz8czhw4O/U1e1zOpyIZl1vY4JQff0h8vPP4fDhFWRk/Jn+/b/ldEgRzRKlMUGmvr6SvLyzqaxcw/jxr9Cv38VOhxTxrOttTJCJju5Ojx5fY8KE1y1JBgmfzihFpBswVFWLAxyPMRGrtnYvDQ0VJCSMZPTop5wOxzTjNVGKyH8BTwDdgOEikg38WFW/Gejg/MVmODfBrqZmN3l5M1F1MXVqIVFRdlUsmPjyr/Eg8DVgGYCq5orIqIBG5UeNM5w3Tt7bOMM5YMnSBIXq6hLy8mZQU7OLSZP+ZkkyCPlyjbJOVctbrNNABBMINsO5CWbV1TvIzZ1Obe1/yMpaQkrKdKdDMq3w5U/XBhG5FIgSkeHAfwPLAxuW/9gM5yaYbdnyQ+rrD5CVtZQePaY5HY5pgy9nlLcAUwAX8CZQjTtZhgSb4dwEs7Fj/0B29qeWJIOcL4lytqr+UFUne173AN8IdGD+YjOcm2Bz5Mh61q//Fg0NR4mJ6UlS0kSnQzJe+JIoH2hl3f3+DiRQ5k5O4+ELJ5KWkoAAaSkJPHzhRBvIMY6orMwnN/cMysuXUVu72+lwjI/avEYpIrOBOUCaiDzR7KMeuLvhIcNmODfBoKJiDXl5s4iKSiA7+yMSEkY6HZLxUXuDOXuBQtzXJNc1W18B2IyhxnTA4cMryc+fTXR0T0+SHOF0SKYD2kyUqroWWCsiL6pqdRfGZEzYiY5OIjExg/HjXyY+fpjT4ZgO8uX2oDQR+TkwHohvXKmqYwIWlTFh4ujRYhISRtK9+3gmT/4MEXE6JNMJvgzmLAT+BAju0e6/Aq8EMCZjwsKBA0tZvXoSpaXu57YtSYYuXxJloqouAVDVLar6AGD1MY1px/7971FQcC4JCaPp18/mkgx1vnS9a8T9p3CLiNwIlAL9AhuWMaFr3763WbfuErp3n0hW1gfExvZxOiRzgnxJlLcDScBtwM+BnsC3AxmUMaGqpuY/rF9/OUlJJzFp0t+JjU1xOiTjB14Tpaqu8LytAK4GEJHBgQzKmFAVFzeAzMxF9OhxCjExPZwOx/hJu9coRWSqiMwVkb6e5Qki8gI+ToohInNEZJOIFItIq/deisilIrJeRNaJyEsdPgJjgsDu3QspK3sLgN69Z1uSDDNtJkoReRh4EbgS+LuI3I97Tso8wOutQSISDTyDe6R8PHCFiIxv0WY0cC/wdVWdAHy/k8dhjGN27VrApk3XsXv3/6EaMjMQmg5or+t9AZClqlUi0hvY5Vn2dSLHaUCxqm4FEJFXPNtc36zNd4BnVPUggKru7egBGOOkkpKnKS6+ld69z2HChNftFqAw1V6irFbVKgBVPSAiGzuQJAHSgJ3Nlktwz5Te3BgAEfkMiAZ+oqp/78A+fGKlIEwg7Nz5S7ZsuYs+fS5gwoRXiYqKczokEyDtJcoRIvKm570A6c2WUdULvWy7tT+tLfslMcBo4AxgMPBPEclsOaO6iNwA3AAwdOhQL7s9lpWCMIFSU1NCauolZGS8SFRUrNPhmABqL1Fe1GL56Q5uuwQY0mx5MO7ue8s2y1W1DtgmIptwJ85VzRup6gJgAUBOTk6HLgK1VwrCEqXpKFWlrm4f3bqlMnLkE6g2WI2bCNDepBgfnuC2VwGjPeUjSoHLgZaPKCwCrgAWekbWxwBbT3C/x7BSEMZfVJVt2+7nP//5E1OmfEFc3CBELElGAl8eYewUVa3HXUZiCbAB+KuqrhORB0XkfE+zJcB+EVmPe0T9blXd7884rBSE8QdVZcuWu/jyy4fp0+cCunUb4HRIpgsF9M+hqr4HvNdi3f80e6/AHZ5XQNw9e+wx1yjBSkGYjlF1UVz835SWPk1a2q2MGvVrG92OMD4nShGJU9WaQAYTCI3XIW3U23RWSclTlJY+zZAhdzFixKOWJCOQ10QpItOA/8P9jPdQEckCrlfVWwMdnL9YKQhzIgYOvJ7o6O4MHHi9JckI5cs1yqeAc4H9AKqah02zZsKcy1XP9u0/o76+gpiYJAYN+o4lyQjmS6KMUtUdLdY1tNrSmDDgctWxYcMVbN/+I/bvX+x0OCYI+HKNcqen+62e57dvBTYHNixjnOFy1bBu3aXs37+YkSOfoH//K50OyQQBXxLlTbi730OBPcBSzzpjwkpDQxXr1l3EgQPvM3r006Sl3ex0SCZI+JIo61X18oBHYozDamv3UFmZz5gxCxg06DtOh2OCiC+JcpXn0cJXgTdVtSLAMRnTpRoaqoiKiichIZ1p0zYSE5PkdEgmyHgdzFHVkcDPgClAgYgsEhE7wzRhob7+MHl5s9iy5W4AS5KmVT49wqiq/1bV24CTgMO4J/Q1JqTV1R0kL28WFRUr6NnzFKfDMUHMa6IUkSQRuVJE3gFWAmXAqQGPzJgAqqvbT17eTCorc5kw4Q1SU1tOlmXMV3y5RlkIvAM8qqr/DHA8xgScqov8/DkcObKezMy36dNnjtMhmSDnS6IcoaqugEdiTBcRiWLo0PuJjk6id++znA7HhIA2E6WI/FJV7wTeEJHjJsv1YYZzY4JKdXUJlZVf0LfvBaSmznU6HBNC2jujfNXz347ObB50rGaOqaraTl7eDOrrD3HyyVuJienpdEgmhLQ3w/lKz9sMVT0mWYrILcCJzoDeJaxmjqmq2kJu7gwaGg4zadISS5Kmw3y5Pejbrayb7+9AAqW9mjkm/B09uom1a6fT0FBJVtaH9OgxzemQTAhq7xrlZbjr3AxvXn0RSAbKW/9W8LGaOZGtrOxNVGvJzv6YpKSJTodjQlR71yhX4p6DcjDwTLP1FcDaQAblT4NSEihtJSlazZzwptqASDRDh97DgAHXERdnNW5M57XZ9VbVbaq6VFWnquqHzV4rPeVlQ8Lds8eSEBt9zDqrmRPeKirWsGpVJkeOrEdELEmaE9Ze1/sTVZ0uIgeB5rcHCe66YL0DHp0fWM2cyHL48Ary8mYTE5NCVJT1Gox/tNf1biz30LcrAgkkq5kTGcrL/0VBwTnExvYjO/sj4uOHOh2SCRPtdb0bn8YZAkSragNwCvBdoHsXxGaMzyoqviA/fw7dug1i8uRPLEkav/Ll9qBFuMtAjAReADKAlwIalTEdlJg4nv79ryI7+2Pi4qz3YPzLl0Tp8gzeXAj8ylOm1n4TTVAoL/+E+vpDREcnMHbs723gxgSEL4myXkQuAa4G3vWsiw1cSMb4Zt++tz2T7v7A6VBMmPP1yZwzcU+ztlVEhgMvBzYsY9q3d+9rrFt3MUlJJzFixP86HY4Jc16nWVPVQhG5DRglIuOAYlX9eeBDM6Z1e/a8xIYNV9Oz56lMnPg3YmJ6OB2SCXNeE6WI/D/gz0Ap7nsoB4jI1ar6WaCDM6alhoZqtm27n5SU6WRmLrYaN6ZL+DJx75PAOaq6HkBEMnAnzpxABmZMS6pKdHQ82dkfExubSnR0otMhmQjhyzXKbo1JEkBVNwDdAheSMccrKXmaoqLvoarExw+zJGm6lC+Jco2I/EFETvO8fkcITYphQt/OnU9QXHwrNTW7Ua13OhwTgXzpet8I3Ab8APc1yk+B3wQyKGMa7djxMNu23Udq6iVkZLxIVJTdmWa6XruJUkQmAiOBt1T10a4JyRi3HTt+zrZtD9Cv35WMG7eQqChf/q4b43/tzR50H+6ZzNcAU0XkQVV9rssi8yOrmROakpKmMHDgdxkz5hlEor1/wZgAae9P9JXAJFU9IiKpwHtAyCVKq5kTWlSViopV9OgxjT595ljNbRMU2hvMqVHVIwCqWualbdCymjmhQ9VFUdGtrFlzMocPr/T+BWO6SHtnlCOa1coRYGTz2jmhUtfbauaEBlUXmzffyO7df2Tw4DtJTp7qdEjGNGkvUV7UYjkk63tbzZzgp9rAxo3z2bPneYYOvY/hw3+GiDgdljFN2pu498P2Xr5sXETmiMgmESkWkXvaaXexiKiI+P1pH6uZE/z27XuHPXueJz39p5YkTVAK2P0W4h6mfAaYBZQAq0RkcfOnfDztknHfp7kiEHFYzZzgl5o6l+zsT0hJOd3pUIxpVSBvTJuGe6ahrQAi8gpwAbC+RbuHgEeBuwIViNXMCT4uVw2bNt3A4MG3k5ycbUnSBDWfR7JFJK6D204DdjZbLqHFzOgiMhkYoqrvYiJGQ0MVhYVz2bPnBSoqVjsdjjFeeU2UIjJNRAqAIs9yloj48ghjaxeamsreikgU7pmJ7vQhhhtEZLWIrC4rK/Nh1yZYNTQcoaDgPA4cWMKYMX9k0KDrnQ7JGK98OaN8CjgX2A+gqnl8Vcq2PSW4Kzg2GgzsaracDGQCH4vIduBkYHFrAzqqukBVc1Q1JzU11Yddm2BUX19Jfv45lJcvY9y4hZYkTcjwJVFGqeqOFusaWm15rFXAaBEZLiLdgMuBxY0fquohVe2rqumqmg4sB85XVeuLhamoqFhiYnqQkfEXBgy4xulwjPGZL4M5O0VkGu6StdHArcBmb19S1XoRuQVYAkQDz6nqOhF5EFitqovb34IJF3V1BwEXsbF9yMxcbLf/mJDjS6K8CXf3eyiwB1jqWeeVqr6H+xnx5uv+p422Z/iyTRNaamv3kZ9/NlFR8Uye/JklSROSfCkuthd3t9mYDqmt3Ute3llUVRUxYcJbliRNyPKluNgfaTZa3UhVbwhIRCYs1NTsJi9vJtXV25k48V169ZrpdEjGdJovXe+lzd7HA9/k2PsjjTnOpk3fprr6SyZNep+UlOlOh2PMCfGl6/1q82UR+TPwj4BFZMLCmDG/p6ZmNz17nux0KMacsM7MMTkcGObvQEzoq6rawpYtP0TVRXz8MEuSJmz4co3yIF9do4wCDgBtzgRkItPRo5vIzZ2Jy1XNoEE3kpAw3OmQjPEbb8XFBMgCSj2rXKp63MCOiWxHjqwnN3cGoGRnL7MkacJOu11vT1J8S1UbPC9LkuYYlZX55OaegUgU2dkfk5Q00emQjPE7X0a9V4rISaq6JuDRBIhVYQycurr9xMSkMHHi30hMHO10OMYERHvlamNUtR44DfiOiGwBjuCeFUhV9aQuivGEWBXGwKit3Ue3bn3p1etMpk5dbzW3TVhrr+vdWAZvLjAWOAe4BLjY89+QYFUY/e/Qoc9YsWIUe/a8DGBJ0oS99n7DBUBVt3RRLAFhVRj96+DBjykoOJe4uDSbldxEjPYSZaqI3NHWh6r6RADi8Turwug/Bw4spbDwfOLjh5OV9SFxcQOcDsmYLtFe1zsaSMI9wW5rr5BgVRj9o6pqOwUF55KQMJrs7I8tSZqI0t4Z5W5VfbDLIgkQq8LoHwkJ6YwZ81v69r2A2Ng+TodjTJfyeo0yHFgVxs7bu/d14uOH0KPH1xg48NtOh2OMI9rretu8WBFuz54XWb/+Mnbs+JnToRjjqDYTpaoe6MpATHDZvftPbNhwNSkp08nIeNnpcIxxVGdmDzJhbteuP7Bp07fp1essJk58l5iYJKdDMsZRlijNMVSVAweW0Lv3OWRmLiY6OtHpkIxxnD1SYZo0NFQRHZ3A+PGNT9zEORyRMcHBzigNADt2/II1a6ZRV3eQqKg4S5LGNGOJMsKpKtu2/YRt2+4nKSmb6OiQeZbAmC5jXe8I5k6S9/Hll48wYMB1jB37R0SivX/RmAhjZ5QRbOfOx/nyy0cYNOhGxo591pKkMW2wM8oI1r//lajWMXTovbirfhhjWmNnlBFG1cWuXc/ictUTFzeIYcPusyRpjBd2RhlBVBvYuHE+e/Y8T0xMCv36Xex0SMaEhIhIlFYzB1yuejZuvIa9e18mPf1BS5LGdEDYJ0qrmQMuVy3r13+LffveYMSIRxg69IdOh2RMSAn7a5RWMweOHt3EwYNLGDnyCUuSxnRC2J9RRnLNHNUGRKJJSprItGmbiYsb6HRIxoSksD+jbKs2TrjXzGloOEp+/hxKS58BsCRpzAkI+0QZiTVz6usryc8/h4MHP7JHEo3xg7DvekWMIhgAABflSURBVEdazZz6+kPk55/D4cMryMh4kf79L3c6JGNCXtgnSoicmjkuVx15eWdTWbmGCRNeJTX1IqdDMiYshH3XO5JERcXSv/+3mDDhTUuSxvhRRJxRhrva2r1UV2+jR4+vMXjwfzsdjjFhx84oQ1xNzW5yc8+goOACGhqOOh2OMWHJzihDWHV1CXl5M6ip2cWkSe9ZfRtjAiSgZ5QiMkdENolIsYjc08rnd4jIehHJF5EPRWRYIOMJJ1VV28nNPZ3a2j1kZX1ASsrpTodkTNgKWKIU9yywzwDfAMYDV4jI+BbN1gI5qjoJeB14NFDxhJvS0l9TX3+QrKyl9Ox5qtPhGBPWAnlGOQ0oVtWtqloLvAJc0LyBqi5T1cYLa8uBwQGMJ6yMGPEoJ520gh49pjodijFhL5CJMg3Y2Wy5xLOuLfOB91v7QERuEJHVIrK6rKzMjyGGliNH1rN27RnU1PyHqKhYEhPHOB2SMREhkImytWmztdWGIlcBOcBjrX2uqgtUNUdVc1JTU/0YYuiorMwnN/cMqqo2U19f7nQ4xkSUQI56lwBDmi0PBna1bCQiZwH3A9NVtSaA8YSsioo15OXNIjo6kaysj0hMHO10SMZElECeUa4CRovIcBHpBlwOLG7eQEQmA38AzlfVvQGMJWRVVHxBbu4MoqOTyc7+1JKkMQ4IWKJU1XrgFmAJsAH4q6quE5EHReR8T7PHgCTgNRHJFZHFbWwuYsXFDSEl5XQmT/6UhIThTodjTEQS1VYvGwatnJwcXb16dYe+E4o1cyoqcunefQJRUbFOh2JMRBCRL1Q1p7XPwv4RxsaaOaXlVShf1cxZtLbU6dDadODAP1i79lS2bXvA6VCMMURAogy1mjn7979HQcF5JCSMYciQu5wOxxhDBCTKUKqZU1a2iMLCuXTvnkl29kd06xaZt0IZE2zCPlGGSs2c+voKNm26nuTkKWRlLSU2trfTIRljPMI+UYZKzZyYmGSysv7BpElLiI1NcTocY0wzYT/NWrDXzNm9+0/U1x9gyJA7SU6e7HQ4xphWhH2ihOCtmbNr1x/YvPlGevU6m8GDv497wiVjTLAJ+653sCop+Q2bN99I797/RWbm25YkjQliligdsHPnLykuvo2+feeSmfkm0dHxTodkjGmHJUoHREV1JzX1MsaP/ytRUd2cDscY44Ulyi6iqlRVbQUgLe1Gxo9/2R5PNCZEWKLsAqrK1q33smpVJkeObABApLXpOo0xwcgSZYCpKlu23MnOnf9L//7XkJgYXPdvGmO8i4jbg5yi6qKo6DZ27XqGtLRbGTXq13YmaUwIsjPKANqz58/s2vUMQ4bcZUnSmBBmZ5QB1L//VURFJZKaerElSWNCmJ1R+pnLVU9x8V1UV5cgEk2/fpdYkjQmxFmi9COXq5b16y+npOSXHDjwntPhGGP8xLrefuJy1bBu3SXs3/8OI0c+yaBBNzgdkjHGTyIiUQa6Zk5DQxXr1l3IgQN/Z/To35KWdpPftm2McV7YJ8rGmjmN5SAaa+YAfkuWLlc1tbV7GDv2WQYOnO+XbRpjgkfYX6MMZM2c+vpKGhqqiY3txUknrbAkaUyYCvtEGaiaOfX1h8jPn82GDVegqvbctjFhLOwTZSBq5tTVHSQvbxYVFSvp3/8qu/3HmDAX9onS3zVzamv3kZc3k8rKPCZMeJPU1Iv8EaYxJoiF/WCOP2vmqCrr11/C0aMbyMx8mz595vg7XGNMEAr7RAn+q5kjIowc+Tj19eX06jXTD5EZY0JB2He9/aG6uoTS0mcASE6eYknSmAgTEWeUJ6Kqajt5eTOoq9tP375ziYsLvmqOxpjAskTZjqqqLeTmzqCh4TBZWUstSRoToSxRtuHo0U3k5s7E5aomK+sjkpMnOx2SMcYhlijbUFGxBmggO3sZSUkTnQ7HGOMgS5QtNDRUER2dQP/+V9Cnz7nExCQ7HZIxxmE26t1MRcUXrFgxkoMHPwKwJGmMASxRNjl0aDm5uTMR6UZ8/HCnwzHGBBHregPl5f+ioOAbxMb2Jzv7I+LjhzodUkSoq6ujpKSE6upqp0MxESQ+Pp7BgwcTG+v7RDYRnyiPHNlAfv5s4uKGkJ39od0C1IVKSkpITk4mPT3dJhYxXUJV2b9/PyUlJQwf7nvPMeK73omJYxk69AdkZ39sSbKLVVdX06dPH0uSpsuICH369OlwLyZiE+WBA0uoqtqGSBTp6T8mLm6A0yFFJEuSpqt15ncuoIlSROaIyCYRKRaRe1r5PE5EXvV8vkJE0gMZT6OysrcoKDiPLVvu7ordGWNCXMASpYhEA88A3wDGA1eIyPgWzeYDB1V1FPAk8L+BiGXR2lK+/shHDL/nb3x3wf9QuO4SkpOnMHbss4HYnQkh0dHRZGdnk5mZyXnnnUd5eXnTZ+vWrWPGjBmMGTOG0aNH89BDD6GqTZ+///775OTkkJGRwbhx47jrrrucOIR2rV27luuvv97pMNq0f/9+zjzzTJKSkrjlllvabHfgwAFmzZrF6NGjmTVrFgcPHgTc1xxvu+02Ro0axaRJk1izZg0AZWVlzJnjv2kQA3lGOQ0oVtWtqloLvAJc0KLNBcDznvevAzPFz32xxuJipeVVnDxwGZeN/jlbysfxJQuJjU3x565MgDX/g/f1Rz5i0drSE95mQkICubm5FBYW0rt3b555xj1LVFVVFeeffz733HMPmzdvJi8vj3//+9/89re/BaCwsJBbbrmFv/zlL2zYsIHCwkJGjBhxwvE0V19ff8Lb+MUvfsGtt97apfvsiPj4eB566CEef/zxdts98sgjzJw5k6KiImbOnMkjjzwCuP9YFRUVUVRUxIIFC7jpJncF1NTUVAYOHMhnn33mlzgDmSjTgJ3Nlks861pto6r1wCGgjz+DaCwuJjQwc9i7bDyQyaOrfspjH5z4/2Sm6zT/g6d8VU3TH8my0SmnnEJpqXt7L730El//+tc5++yzAUhMTOTpp59u+h/00Ucf5f7772fcuHEAxMTE8L3vfe+4bVZWVnLdddcxceJEJk2axBtvvAFAUlJSU5vXX3+da6+9FoBrr72WO+64gzPPPJO7776b9PT0Y85yR40axZ49eygrK+Oiiy5i6tSpTJ06tdWEUFFRQX5+PllZWQCsXLmSU089lcmTJ3PqqaeyaZO7wN7ChQu55JJLOO+885qO97HHHmPq1KlMmjSJH//4x03bnDt3LlOmTGHChAksWLCgEz/lY3Xv3p3TTjuN+Pj4dtu9/fbbzJs3D4B58+axaNGipvXXXHMNIsLJJ59MeXk5u3fvbor1xRdfPOEYIbC3B7V2ZqidaIOI3ADcADB0aMfucWwsIqZE88Tqn1LviqHWFX/CxcVM12qvmqY/JmVuaGjgww8/ZP58dyXNdevWMWXKlGPajBw5ksrKSg4fPkxhYSF33nmn1+0+9NBD9OzZk4ICd4nkxi5jezZv3szSpUuJjo7G5XLx1ltvcd1117FixQrS09Pp378/3/rWt7j99ts57bTT+PLLL5k9ezYbNmw4ZjurV68mMzOzaXncuHF8+umnxMTEsHTpUu67776mxP3555+Tn59P7969+eCDDygqKmLlypWoKueffz6ffvopp59+Os899xy9e/emqqqKqVOnctFFF9Gnz7HnNrfffjvLli077rguv/xy7rnnuKEKn+zZs4eBAwcCMHDgQPbu3QtAaWkpQ4YMaWo3ePBgSktLGThwIDk5OTzwwAOd2l9LgUyUJcCQZsuDgV1ttCkRkRigJ3Cg5YZUdQGwACAnJ+e4RNqeQSkJlHqS4tH6pGPWm9ARqGqaVVVVZGdns337dqZMmcKsWbMA97Wvtq4CdeTq0NKlS3nllVealnv16uX1O5dccgnR0e46T5dddhkPPvgg1113Ha+88gqXXXZZ03bXr1/f9J3Dhw9TUVFBcvJXj93u3r2b1NTUpuVDhw4xb948ioqKEBHq6uqaPps1axa9e/cG4IMPPuCDDz5g8mT3jFmVlZUUFRVx+umn89RTT/HWW28BsHPnToqKio5LlE8++aRvPxw/aH7NuFHjv0+/fv3YtatlyumcQHa9VwGjRWS4iHQDLgcWt2izGJjneX8x8JG2duQnwN/FxYwzAlFNE766Rrljxw5qa2ubrlFOmDCB1atXH9N269atJCUlkZyczIQJE/jiiy+8br+thNt8Xct7+rp37970/pRTTqG4uJiysjIWLVrEhRdeCIDL5eLzzz8nNzeX3NxcSktLj0mSjcfWfNs/+tGPOPPMMyksLOSdd9455rPm+1RV7r333qZtFxcXM3/+fD7++GOWLl3K559/Tl5eHpMnT271fsTbb7+d7Ozs416Nly06o3///k1d6t27d9OvXz/AfQa5c+dXV/hKSkoYNGgQ4P65JiT454QoYInSc83xFmAJsAH4q6quE5EHReR8T7P/A/qISDFwB9C58/J2zJ2cxsMXTiQtJQEB0lISePjCiX7prpmuE+g/eD179uSpp57i8ccfp66ujiuvvJJ//etfLF26FHCfed5222384Ac/cMdz99384he/YPPmzYA7cT3xxBPHbffss8/m6aefblpu7Hr379+fDRs2NHWt2yIifPOb3+SOO+4gIyOj6eyt5XZzc3OP+25GRgbFxcVNy4cOHSItzf17v3Dhwjb3OXv2bJ577jkqKysBd/d27969HDp0iF69epGYmMjGjRtZvnx5q99/8sknm5Js81dnu90A559/Ps8/7x73ff7557ngggua1r/wwguoKsuXL6dnz55NXfTNmzcfc+nhhKhqSL2mTJmiJjysX7++Q+3fWlOipz78oab/8F099eEP9a01JSccQ/fu3Y9ZPvfcc/WFF15QVdX8/HydPn26jhkzRkeOHKk/+clP1OVyNbV955139KSTTtJx48ZpRkaG3nXXXcdtv6KiQq+55hqdMGGCTpo0Sd944w1VVX3ttdd0xIgROn36dL355pt13rx5qqo6b948fe21147ZxqpVqxTQhQsXNq0rKyvTSy+9VCdOnKgZGRn63e9+t9Xjy8zM1MOHD6uq6r///W8dPXq0nnrqqfrAAw/osGHDVFX1T3/6k958883HfO9Xv/qVZmZmamZmpp588slaXFys1dXVOmfOHJ04caJefPHFOn36dF22bJmXn7B3w4YN0169emn37t01LS1N161bp6qq8+fP11WrVqmq6r59+3TGjBk6atQonTFjhu7fv19VVV0ul37ve9/TESNGaGZmZlN7VdXHHntMn3rqqVb32drvHrBa28g7ov7t6QZcTk6OtuwSmdC0YcMGMjIynA4jrD355JMkJycH9b2UgXL66afz9ttvt3pduLXfPRH5QlVzWttWxD7CaEwkuOmmm4iLi3M6jC5XVlbGHXfc4dPgmS8sURoTxuLj47n66qudDqPLpaamMnfuXL9tzxKlcVSoXfoxoa8zv3OWKI1j4uPj2b9/vyVL02XUMx+ltyeBWor4iXuNcwYPHkxJSQllZWVOh2IiSOMM5x1hidI4JjY2tkOzTBvjFOt6G2OMF5YojTHGC0uUxhjjRcg9mSMiZcCOTn69L7DPj+E4xY4jeITDMYAdB8AwVU1t7YOQS5QnQkRWt/WIUiix4wge4XAMYMfhjXW9jTHGC0uUxhjjRaQlyhMv8hEc7DiCRzgcA9hxtCuirlEaY0xnRNoZpTHGdFhYJkoRmSMim0SkWESOm39eROJE5FXP5ytEJL3ro/TOh+O4Q0TWi0i+iHwoIsOciLM93o6hWbuLRURFJChHXn05DhG51PPvsU5EXurqGH3hw+/UUBFZJiJrPb9X5zgRZ3tE5DkR2SsihW18LiLylOcY80XkpBPeaVtTn4fqC4gGtgAjgG5AHjC+RZvvAb/3vL8ceNXpuDt5HGcCiZ73NwXbcfhyDJ52ycCnwHIgx+m4O/lvMRpYC/TyLPdzOu5OHscC4CbP+/HAdqfjbuU4TgdOAgrb+Pwc4H3c5bBPBlac6D7D8YxyGlCsqltVtRZ4BbigRZsLgOc9718HZkpHapB2Da/HoarLVPWoZ3E57pLAwcSXfwuAh4BHgeNL+gUHX47jO8AzqnoQQFX3dnGMvvDlOBTo4Xnfk+NLTDtOVT+llbLWzVwAuAsfqS4HUkRk4InsMxwTZRqws9lyiWddq23UXS3yENCH4OLLcTQ3H/df0WDi9RhEZDIwRFXf7crAOsiXf4sxwBgR+UxElovInC6Lzne+HMdPgKtEpAR4D7i1a0Lzq47+v+NVOE6z1tqZYcuhfV/aOM3nGEXkKiAHmB7QiDqu3WMQkSjgSeDargqok3z5t4jB3f0+A/eZ/T9FJFNVywMcW0f4chxXAAtV9ZcicgrwZ89xuAIfnt/4/f/vcDyjLAGGNFsezPHdh6Y2IhKDu4vR3qm8E3w5DkTkLOB+4HxVremi2Hzl7RiSgUzgYxHZjvt60uIgHNDx9XfqbVWtU9VtwCbciTOY+HIc84G/Aqjq50A87uenQ4lP/+90RDgmylXAaBEZLiLdcA/WLG7RZjEwz/P+YuAj9VwFDiJej8PTbf0D7iQZjNfE2j0GVT2kqn1VNV1V03FfZz1fVYOtHrEvv1OLcA+uISJ9cXfFt3ZplN75chxfAjMBRCQDd6IMtSnoFwPXeEa/TwYOqeruE9qi0yNYARoVOwfYjHuE737Pugdx/08I7n/814BiYCUwwumYO3kcS4E9QK7ntdjpmDt6DC3afkwQjnr7+G8hwBPAeqAAuNzpmDt5HOOBz3CPiOcCZzsdcyvH8DKwG6jDffY4H7gRuLHZv8UznmMs8MfvlD2ZY4wxXoRj19sYY/zKEqUxxnhhidIYY7ywRGmMMV5YojTGGC8sURqvRKRBRHKbvdLbaZve1qwuHdznx55ZbvI8jwWO7cQ2bhSRazzvrxWRQc0+e1ZExvs5zlUiku3Dd74vIoknum/TdSxRGl9UqWp2s9f2LtrvlaqahXsCk8c6+mVV/b2qvuBZvBYY1Oyz61V1vV+i/CrO3+JbnN8HLFGGEEuUplM8Z47/FJE1nteprbSZICIrPWeh+SIy2rP+qmbr/yAi0V529ykwyvPdmZ65Egs88xLGedY/0mxuzsc9634iIneJyMW4n4V/0bPPBM+ZYI6I3CQijzaL+VoR+U0n4/ycZpMviMjvRGS1uOen/Kln3W24E/YyEVnmWXe2iHzu+Tm+JiJJXvZjupglSuOLhGbd7rc86/YCs1T1JOAy4KlWvncj8GtVzcadqEo8j8VdBnzds74BuNLL/s8DCkQkHlgIXKaqE3FPRHGTiPQGvglMUNVJwM+af1lVXwdW4z7zy1bVqmYfvw5c2Gz5MuDVTsY5B/ejjI3uV3fp1EnAdBGZpKpP4X7u+ExVPdPzuOMDwFmen+Vq4A4v+zFdLBxnDzL+V+VJFs3FAk97rsk14H62uaXPgftFZDDwpqoWichMYAqwyjMFaALupNuaF0WkCtiOe7qvscA2Vd3s+fx54GbgadxzWT4rIn8DfJ6yTVXLRGSr55ngIs8+PvNstyNxdsc9MW7z2bQvFZEbcP9/NhD344H5Lb57smf9Z579dMP9czNBxBKl6azbcT9nnoW7Z3LcpLuq+pKIrAD+C1giItfjfg73eVW914d9XKnNJsgQkVbnDFXVehGZhnsyh8uBW4AZHTiWV4FLgY3AW6qq4s5aPseJ+9noR3A/Y3yhiAwH7gKmqupBEVmIe46BlgT4h6pe0YF4TRezrrfprJ7AbnXPU3g17rOpY4jICGCrp7u5GHcX9EPgYhHp52nTW3yv9bMRSBeRUZ7lq4FPPNf0eqrqe7gHSlobea7APa1ba94E5uKei/FVz7oOxamqdbi70Cd7uu09gCPAIRHpD3yjjViWA19vPCYRSRSR1s7OjYMsUZrO+i0wT0SW4+52H2mlzWVAoYjkAuNwT8+/HndC+UBE8oF/4O6WeqWq1cB1wGsiUgC4gN/jTjrverb3Ce6z3ZYWAr9vHMxpsd2DuGf9GaaqKz3rOhyn59rnL4G7VDUPdw2ddcBzuLvzjRYA74vIMlUtwz0i/7JnP8tx/6xMELHZg4wxxgs7ozTGGC8sURpjjBeWKI0xxgtLlMYY44UlSmOM8cISpTHGeGGJ0hhjvLBEaYwxXvx/KQqcxJzoOhkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# your code here\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ft_df = fitted_cv1.bestModel.stages[0].summary.roc\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(ft_df.select('FPR').collect(),\n",
    "         ft_df.select('TPR').collect(), label='ROC curve (area = %0.2f)'% roc)\n",
    "plt.plot([0, 1], [0, 1], 'y--')\n",
    "#plt.xlim([-0.01, 1.0])\n",
    "#plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 8\n",
    "Create a new cross validator object named cv2 similar to cv1 but this time add a ParamGridBuilder.  Define a grid of elastic net regularization parameters. Fit cv2 and name the resulting fitted cross validator fitted_cv2.  The number of parameters in your grid should be limited such that it runs in a reasonable amount of time (around 5 to 10 minutes max).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "grid = ParamGridBuilder().addGrid(logistic.regParam, [0.01, 0.5,2.0]).addGrid(logistic.elasticNetParam, [0.0, 0.5, 0.1]).build()\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='spam', metricName='areaUnderROC')\n",
    "cv2 = CrossValidator(estimator=pipe1, estimatorParamMaps=grid,numFolds=3,evaluator=evaluator,seed=70)\n",
    "fitted_cv2 = cv2.fit(spam_df3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 9\n",
    "- Print the resulting AUC from fitted_cv2. \n",
    "- Print the best model's L1 and L2 regularization parameters\n",
    "- Analyze the L1 feature selection:\n",
    "    - Print the total number of features\n",
    "    - Print the number of features that L1 regularization eliminated\n",
    "    - If any features were eliminated, print a sample of 10 words that were eliminated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Area under ROC Curve:1.0\n",
      "2. Regularization parameter:  0.5\n",
      "3. Elastic net regularization parameter:  0.0\n",
      "4. Total number of features:  13361\n",
      "5. Number of features that L1 regularization eliminated:  167\n",
      "6. 10 Words eliminated by L1:  ['wer', 'girl??', '\"storming', 'phne,', '\"height', 'grahmbell', 'girl..!', 'telphone...', 'invnted', '\"short']\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "roc2 = evaluator.evaluate(fitted_cv2.transform(spam_df3))\n",
    "print(f\"1. Area under ROC Curve:{roc2}\")\n",
    "\n",
    "bestPipe = fitted_cv2.bestModel\n",
    "bestLRModel = bestPipe.stages[0]\n",
    "bestParams = bestLRModel.extractParamMap()\n",
    "bestParams\n",
    "print(\"2. Regularization parameter: \", bestParams[bestLRModel.getParam(\"regParam\")])\n",
    "print(\"3. Elastic net regularization parameter: \", bestParams[bestLRModel.getParam(\"elasticNetParam\")])\n",
    "\n",
    "voc1 = p_transformer.stages[0].vocabulary\n",
    "print(\"4. Total number of features: \", len(voc1))\n",
    "\n",
    "weights = fitted_cv2.bestModel.stages[0].coefficients.toArray()\n",
    "print(\"5. Number of features that L1 regularization eliminated: \", len(weights[weights == 0]))\n",
    "w = list(weights)\n",
    "p = {}\n",
    "for i in range(len(w)):\n",
    "    p[voc1[i]] = fabs(w[i])\n",
    "    \n",
    "e_words = []\n",
    "for item  in p.items():\n",
    "    if item[1] == 0:\n",
    "        e_words.append(item[0])\n",
    "print(\"6. 10 Words eliminated by L1: \", e_words[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading Feedback\n",
    "-3 Multiple problems\n",
    "- We were looking for AUC from the cross validation but would have accpted AUC from test data too.  You used training data which is probably why the accuracy is 100%.<br>\n",
    "- We were looking for the actual L1/L2 multipliers not alpha and lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 10\n",
    "Analyze the best model weights in fitted_cv2.  Print the 10 words that contribute the most to predicting spam.  Print the 10 words that contribute the least to predicting spam.  Do the words make sense?  Explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words that contribute the most:\n",
      "\n",
      "1. Word:07090201529,  Weight:0.20788513001455278\n",
      "2. Word:sexy?,  Weight:0.18330060607843854\n",
      "3. Word:Â£12,  Weight:0.17754713185425383\n",
      "4. Word:0207-083-6089,  Weight:0.17746146392909284\n",
      "5. Word:08714712388,  Weight:0.17269768305895794\n",
      "6. Word:stories,  Weight:0.17026475676389002\n",
      "7. Word:08718738034.,  Weight:0.17019762679882186\n",
      "8. Word:08712400200.,  Weight:0.17019762679882186\n",
      "9. Word:08715205273,  Weight:0.17019762679882186\n",
      "10. Word:gbp/sms,  Weight:0.16687348522733023\n",
      "\n",
      "Words that contribute the least:\n",
      "\n",
      "1. Word:wer,  Weight:0.0\n",
      "2. Word:girl??,  Weight:0.0\n",
      "3. Word:\"storming,  Weight:0.0\n",
      "4. Word:phne,,  Weight:0.0\n",
      "5. Word:\"height,  Weight:0.0\n",
      "6. Word:grahmbell,  Weight:0.0\n",
      "7. Word:girl..!,  Weight:0.0\n",
      "8. Word:telphone...,  Weight:0.0\n",
      "9. Word:invnted,  Weight:0.0\n",
      "10. Word:\"short,  Weight:0.0\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "most_words = sorted(p.items(), key=lambda x: x[1], reverse=True)[0:10]\n",
    "least_words = sorted(p.items(), key=lambda x: x[1], reverse=False)[0:10]\n",
    "\n",
    "print(\"Words that contribute the most:\\n\")\n",
    "for i in range(10):\n",
    "    print(f\"{i + 1}. Word:{most_words[i][0]},  Weight:{most_words[i][1]}\")\n",
    "\n",
    "\n",
    "print(\"\\nWords that contribute the least:\\n\")\n",
    "for i in range(10):\n",
    "    print(f\"{i + 1}. Word:{least_words[i][0]},  Weight:{least_words[i][1]}\")    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your comments here: The words that have 0.0 weights associated contribute the least. The Word:07090201529 has the highest weight hence contributes the most in predicting spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading Feedback\n",
    "Your words don't look correct.  I'm wondering if it is because you didn't remove stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extra Credit (5 pts)**  This question is optional.  If you choose to answer this question, you will earn 5 extra credit points.  If you choose not to answer this question, no points will be deducted from your score.  Solve the following equation for $c$ symbolically using the python sympy package.  Convert the solved symbolic solution to a latex format (this can be done with a pyton call), then populate the solution cell with the resulting latex code so that your solution shows up symbolically similar the equation below.\n",
    "\n",
    "$$c g - c h + e \\left(a + 1\\right)^{b} - \\frac{d \\left(\\left(a + 1\\right)^{b} - 1\\right)}{a} + \\frac{f \\left(\\left(a + 1\\right)^{b} - 1\\right)}{a} = 0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not delete or change this cell\n",
    "import pyspark.sql.functions as fn\n",
    "# if running on data bricks\n",
    "if is_databricks():\n",
    "    # install sympy\n",
    "    dbutils.library.installPyPI\n",
    "    dbutils.library.installPyPI('sympy')\n",
    "    print(dbutils.library.list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\left[ \\frac{- a e \\left(a + 1\\right)^{b} + d \\left(a + 1\\right)^{b} - d - f \\left(a + 1\\right)^{b} + f}{a \\left(g - h\\right)}\\right]\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "import sympy as sp\n",
    "from sympy import Eq, solve\n",
    "c, g, h, a, b, d, e, f = sp.symbols('c g h a b d e f')\n",
    "equat = ((((c*g) - (c*h) + (e*((a + 1)**b))))-((d*(((a + 1)**b) - 1))/a)+((f*(((a + 1)**b) - 1))/a))\n",
    "equation = Eq(equat)\n",
    "s=solve(equation, c)\n",
    "print(sp.latex(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add your latex output here such that a human readable equation is displayed for grading:\n",
    "$$\\left[ \\frac{- a e \\left(a + 1\\right)^{b} + d \\left(a + 1\\right)^{b} - d - f \\left(a + 1\\right)^{b} + f}{a \\left(g - h\\right)}\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading Feedback\n",
    "+5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
